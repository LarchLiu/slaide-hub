---
page: 1

theme: seriph
background: https://cover.sli.dev
title: "在笔记本上运行1000亿参数AI模型：BitNet.cpp的秘密"
titleTemplate: '%s - Slaide'
layout: cover
presenter: dev
seoMeta:
  ogTitle: "在笔记本上运行1000亿参数AI模型：BitNet.cpp的秘密"
addons:
  - slidev-theme-viplay
subtitlesConfig:
  noTTSDelay: 2000
  ttsApi: "https://edgetts.deno.dev/v1/audio/speech"
  ttsLangName:
    en: "English(US)"
    zh_CN: "中文(简体)"
  apiCustom:
    voice: 'rate:-0.2|pitch:0.1'
  ttsModel:
    zh_CN:
      - value: "zh-CN-YunjianNeural"
        display: "云间"
      - value: "zh-CN-XiaoxiaoNeural"
        display: "晓晓"
    en:
      - value: "en-US-AndrewNeural"
        display: "Andrew"
      - value: "en-US-AriaNeural"
        display: "Aria"
subtitles: {"default":{"zh_CN":["哈喽大家好！","今天聊的话题有点像科幻成真了。","想象一下，在你自己的笔记本上，","没有几万块的显卡，","不用租昂贵的云服务器，","就靠你笔记本里的CPU，","能跑一个1000亿参数的大型AI模型！","听起来是不是挺夸张的？"],"en":["Hello everyone!","Today's topic is a bit like science fiction coming true.","Imagine this: on your own laptop,","without a graphics card that costs tens of thousands,","without renting expensive cloud servers,","just using the CPU inside your laptop,","you can run a large AI model with 100 billion parameters!","Doesn't that sound pretty exaggerated?"]}}
---

# 用笔记本跑1000亿参数AI模型？

## Microsoft BitNet.cpp 如何实现

---
page: 2

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["但微软最近真的把这个变成了现实，","他们开源了一个叫做 BitNet.cpp 的框架。","我试了试，当时就惊呆了！","它不仅让这成为可能，","而且还非常快，非常高效。","为什么说这个 BitNet.cpp 这么牛呢？"],"en":["But Microsoft recently made this a reality.","They open-sourced a framework called BitNet.cpp.","I tried it, and I was absolutely stunned!","Not only does it make this possible,","but it's also incredibly fast and efficient.","So, why is this BitNet.cpp so impressive?"]}}
---

# BitNet.cpp: 让AI模型在CPU上飞跃

- Microsoft 开源框架
- 实现大型AI模型在CPU上高效运行
- 快速、高效，改变AI应用模式

![BitNet.cpp Logo](/bMI9j/e34a22f676079b3f8fc351814d7e70a7bdf8136f.webp)

---
page: 3

layout: image-right
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["就在几个星期前，","我还在用笔记本倒腾小小的AI模型，","机器就吭哧吭哧地喘粗气。","想跑大一点的？","没门儿，得上云，得花钱。","结果我看到了 bitnet.cpp，","哇塞，感觉像有人给我变了个魔法。","这东西最大的亮点就是：","不需要高端硬件！","你的日常笔记本或台式机就能搞定。","完全不用为了跑AI去卖肾买显卡了，哈哈。"],"en":["Just a few weeks ago,","I was still fiddling with small AI models on my laptop,","and the machine was wheezing and struggling.","Want to run something bigger?","No way, you had to go to the cloud, and pay for it.","Then I saw BitNet.cpp,","Wow, it felt like someone performed magic for me.","The biggest highlight of this thing is:","No high-end hardware needed!","Your everyday laptop or desktop can handle it.","You absolutely don't need to sell a kidney to buy a graphics card for running AI anymore, haha."]},"click1":{"zh_CN":["这东西最大的亮点就是：","不需要高端硬件！","你的日常笔记本或台式机就能搞定。","完全不用为了跑AI去卖肾买显卡了，哈哈。"],"en":["The biggest highlight of this is:","No high-end hardware required!","Your everyday laptop or desktop can handle it.","You definitely don't need to sell a kidney to buy a graphics card just to run AI anymore, haha."]}}
---

# 告别硬件束缚

- **痛点**: 跑AI依赖昂贵GPU/云服务

<div v-click="1">

- **BitNet.cpp**: 在普通CPU上运行大规模模型

![BitNet.cpp CPU Advantage](/bMI9j/64fa6163d0ae9f58cd49ef0865f481a7581d7ca3.webp)

</div>

---
page: 4

layout: two-cols
subtitles: {"default":{"zh_CN":["而且，它的速度提升非常惊人："],"en":["Moreover, its speed improvement is astonishing:"]},"click1":{"zh_CN":["在 ARM 架构的 CPU 上，","速度能提升 1.37到5.07倍。","在 x86 架构的 CPU 上，","更是能飙到 2.37到6.17倍！","而且模型越大，提速越明显。"],"en":["On ARM-based CPUs,","the speed can increase by 1.37 to 5.07 times.","On x86-based CPUs,","it can even soar to 2.37 to 6.17 times!","And the larger the model, the more significant the speed boost."]},"click2":{"zh_CN":["更厉害的是，它还超级省电：","能节省 55.4%到82.2%的能源消耗。","这不仅帮你省电费，对地球也更友好。"],"en":["What's more impressive is that it's incredibly power-efficient:","It can save 55.4% to 82.2% of energy consumption.","This not only saves you electricity bills but is also friendlier to the planet."]},"click3":{"zh_CN":["还有一个点我特别喜欢，就是隐私性。","因为模型可以在本地运行，","你的数据就不用上传到云端，","更安全，更放心。"],"en":["Another point I particularly like is privacy.","Because the model can run locally,","your data doesn't need to be uploaded to the cloud,","making it safer and more secure."]},"click4":{"zh_CN":["而且它真的能处理大型号的模型。","我跑了一下那个1000亿参数的模型，","它输出文本的速度大概每秒5-7个词，","差不多就是你我平时读邮件的速度，","感觉就像跟一个真人聊天一样，非常实用。"],"en":["And it can really handle large models.","I ran the 100 billion parameter model,","and its text output speed is about 5-7 words per second,","which is roughly the speed you and I read emails,","It feels just like chatting with a real person, very practical."]}}
---

# BitNet.cpp 核心优势

- **无需高端硬件**: 利用现有CPU

<div v-click="1">

- **速度惊人**:
  - ARM CPU: 1.37x - 5.07x 更快
  - x86 CPU: 2.37x - 6.17x 更快 (模型越大提升越明显)

</div>

<div v-click="2">

- **能效比高**: 节省 55.4% - 82.2% 能源

</div>

<div v-click="3">

- **数据隐私**: 本地离线运行

</div>

<div v-click="4">

- **支持大型模型**: 100B参数模型，每秒5-7词输出

</div>

::right::



---
page: 5

layout: two-cols
subtitles: {"default":{"zh_CN":["这到底是怎么做到的呢？","说到技术原理，","BitNet.cpp 专门用来运行 1-bit 大语言模型。","比如那个 BitNet b1.58 模型。","传统的AI模型就像那些大型SUV，","强劲但又重又费油，","需要用大块头的数据来存储知识，","非常占用内存，也需要特别强的硬件。"],"en":["So, how is this actually done?","Speaking of the technical principles,","BitNet.cpp is specifically designed to run 1-bit large language models.","For example, the BitNet b1.58 model.","Traditional AI models are like those large SUVs,","powerful but heavy and fuel-hungry,","requiring large chunks of data to store knowledge,","taking up a lot of memory and needing particularly strong hardware."]},"click1":{"zh_CN":["而 BitNet.cpp 配合的1-bit模型呢，","就像一个轻巧灵活的电动滑板车。","它用了个叫 1.58-bit 量化的技术。","理解成一种超级压缩方法，","把模型里复杂的数字都变成了最简单的 -1、0、1。","这一下就把模型体积大大缩小了，","小到你的CPU都能轻松搬动，处理起来毫不费力。"],"en":["And the 1-bit model paired with BitNet.cpp,","is like a light and nimble electric scooter.","It uses a technique called 1.58-bit quantization.","Think of it as a super compression method,","which turns the complex numbers inside the model into the simplest -1, 0, and 1.","This significantly shrinks the model size,","so small that your CPU can easily move and process it without effort."]},"click2":{"zh_CN":["而且，这个框架里还有很多优化过的核心代码。","这些就像给滑板车的马达做了特别调校，","让它在你的CPU上也能跑得飞快，","而且不浪费能量。"],"en":["Furthermore, there are many optimized core codes in this framework.","These are like specially tuned motors for the scooter,","allowing it to run very fast on your CPU,","and without wasting energy."]},"click3":{"zh_CN":["虽然目前它主要聚焦在CPU上，","但未来也会支持NPU和GPU。","不过就目前来看，","已经足以让你的普通电脑变成AI小超人了。","这项技术带来的可能性也很多：","比如可以在没网时用的离线语音助手、","让AI跑在各种小型设备上，","还有就是刚才说的，更环保的AI。"],"en":["Although currently it mainly focuses on the CPU,","it will also support NPUs and GPUs in the future.","However, as it stands now,","it's already enough to turn your ordinary computer into a little AI superhero.","The possibilities brought by this technology are also numerous:","For example, offline voice assistants that work without internet,","running AI on various small devices,","and as I just mentioned, more environmentally friendly AI."]}}
---

# 技术核心：1-bit LLMs 与量化

- **传统模型**: 使用16/32位数据，体积庞大，硬件需求高 (类比: 大型SUV)

<div v-click="1">

- **BitNet.cpp + 1-bit LLM (如 BitNet b1.58)**:
  - 使用 **1.58-bit 量化**: 数据压缩到 -1, 0, 1
  - 模型体积大幅缩小 (类比: 电动滑板车)

</div>

<div v-click="2">

- **优化过的核心代码 (Kernels)**: 专为CPU优化计算

</div>

<div v-click="3">

- **目标**: CPU优先，未来支持NPU/GPU

</div>

::right::



---
page: 6

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["好了，光说不练假把式！","想不想亲手试试？","我现在就来带你一起上手。","放心，没你想的那么难，","我尽量用最简单的方式跟你说，","就像给我非技术表弟解释一样，哈哈。"],"en":["Okay, talking without doing is just empty talk!","Do you want to try it yourself?","I'll walk you through getting started right now.","Don't worry, it's not as hard as you might think,","I'll try to explain it in the simplest way possible,","like explaining it to my non-tech cousin, haha."]}}
---

# 亲手实践：上手指南

---
page: 7

layout: two-cols
subtitles: {"default":{"zh_CN":["第一步：准备工具","你需要装几个东西。","别怕，都很基础：","Python: 3.9版本或更新的。","CMake: 3.22版本或更高的。","它像个万能胶，能帮你把代码编译起来。","Clang: 18版本或更高的。","这是个编译器。","Conda (可选): 我个人很推荐用它。","能帮你把Python环境管理得干干净净。"],"en":["Step 1: Prepare the Tools","You need to install a few things.","Don't worry, they are all basic:","Python: version 3.9 or newer.","CMake: version 3.22 or higher.","It's like a universal glue that helps compile your code.","Clang: version 18 or higher.","This is a compiler.","Conda (Optional): I personally highly recommend using it.","It can help you keep your Python environment neat and clean."]},"click1":{"zh_CN":["Windows 的朋友们注意啦：","你需要安装 Visual Studio 2022。","安装时记得勾选这些选项：","使用 C++ 的桌面开发","适用于 Windows 的 C++ CMake 工具","Git for Windows","适用于 Windows 的 C++ Clang 编译器","MSBuild Support for LLVM-Toolset (clang)","之后所有命令行操作，","最好都在 Developer Command Prompt 或 PowerShell for VS2022 里进行。","能省去不少麻烦。"],"en":["Attention Windows users:","You need to install Visual Studio 2022.","Remember to check these options during installation:","Desktop development with C++","C++ CMake Tools for Windows","Git for Windows","C++ Clang Compiler for Windows","MSBuild Support for LLVM-Toolset (clang)","After that, all command-line operations,","are best done in the Developer Command Prompt or PowerShell for VS2022.","It can save a lot of trouble."]},"click2":{"zh_CN":["Linux (比如 Debian 或 Ubuntu) 的朋友：","可以直接运行下面这行命令来安装 Clang：","bash -c \"$(wget -O - https://apt.llvm.org/llvm.sh)\"","正式开始编译项目前，","可以先跑一下 `clang -v` 来确认 Clang 装好了。","Windows 用户也要确认能在 VS2022 的命令行环境里访问到它们。"],"en":["For Linux users (like Debian or Ubuntu):","You can directly run the following command to install Clang:","bash -c \"$(wget -O - https://apt.llvm.org/llvm.sh)\"","Before starting the project compilation,","you can run `clang -v` to confirm that Clang is installed.","Windows users also need to confirm that they can access them in the VS2022 command-line environment."]}}
---

## Step 1: 准备工具

- **Python**: 3.9+
- **CMake**: 3.22+
- **Clang**: 18+
- **Conda** (推荐): 管理Python环境

<div v-click="1">

**Windows (需安装 Visual Studio 2022 并勾选):**
- Desktop development with C++
- C++ CMake Tools for Windows
- Git for Windows
- C++ Clang Compiler for Windows
- MSBuild Support for LLVM-Toolset (clang)
*使用 `Developer Command Prompt` 或 `PowerShell for VS2022`*

</div>

<div v-click="2">

**Linux (Debian/Ubuntu):**
```bash
bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"
```

*验证安装: `clang -v`*

</div>

::right::



---
page: 8

layout: image-right
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["第二步：把代码“ snag ”下来","从 GitHub 上把 bitnet.cpp 的代码下载到你的电脑。","打开终端，输入：","git clone --recursive https://github.com/microsoft/BitNet.git","cd BitNet"],"en":["Step 2: 'Snag' the Code","Download the BitNet.cpp code from GitHub to your computer.","Open your terminal and enter:","git clone --recursive https://github.com/microsoft/BitNet.git","cd BitNet"]}}
---

## Step 2: 克隆代码

打开终端或 PowerShell，运行以下命令：

```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
```

---
page: 9

layout: image-right
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["第三步：搭建环境","如果你用 Conda：","conda create -n bitnet-cpp python=3.9","conda activate bitnet-cpp","pip install -r requirements.txt"],"en":["Step 3: Set up the Environment","If you are using Conda:","conda create -n bitnet-cpp python=3.9","conda activate bitnet-cpp","pip install -r requirements.txt"]},"click1":{"zh_CN":["如果你不用 Conda，","确保当前 Python 环境里装好了 requirements.txt 里的库就行。"],"en":["If you are not using Conda,","just make sure your current Python environment meets the requirements in requirements.txt."]}}
---

## Step 3: 设置环境 (推荐使用 Conda)

```bash
# 创建并激活新环境
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

# 安装依赖
pip install -r requirements.txt
```

<div v-click="1">

*如果不用Conda，请确保当前Python环境满足`requirements.txt`要求*

</div>

---
page: 10

layout: two-cols
subtitles: {"default":{"zh_CN":["第四步：下载一个模型","你可以从 HuggingFace 上下载预训练好的模型文件。","我试了 BitNet-b1.58–2B-4T 这个模型，很适合入门测试：","huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T"],"en":["Step 4: Download a Model","You can download a pre-trained model file from HuggingFace.","I tried the BitNet-b1.58–2B-4T model, which is great for getting started:","huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T"]},"click1":{"zh_CN":["下载下来之后，","你需要用一个脚本来设置一下环境，","并指定模型的目录和量化类型：","python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s","setup_env.py 的 Usage 说明里，","有很多可选的模型仓库和量化类型可以玩。","-md 就是指定模型的本地目录，-q 是量化类型。"],"en":["After downloading it,","you need to use a script to set up the environment,","and specify the model directory and quantization type:","python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s","In the Usage instructions for setup_env.py,","there are many optional model repositories and quantization types to experiment with.","-md specifies the local model directory, and -q is the quantization type."]}}
---

## Step 4: 下载模型

从 HuggingFace 下载预训练模型，例如 BitNet-b1.58–2B-4T:

```bash
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
```


::right::


<div v-click="1">

然后运行 `setup_env.py` 配置环境：

```bash
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s
```

*`-md`: 模型本地目录，`-q`: 量化类型。更多选项请参考 `setup_env.py -h`*

</div>

---
page: 11

layout: two-cols
subtitles: {"default":{"zh_CN":["第五步：运行AI魔法！","现在，该让AI动起来了！","先设置好环境，然后运行推理脚本：","python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s","python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p \"You're my trusty AI sidekick\" -cnv","这行命令会让模型进入聊天模式，","并且以 \"You're my trusty AI sidekick\" 作为开场白。"],"en":["Step 5: Run the AI Magic!","Now, it's time to get the AI running!","First, set up the environment, then run the inference script:","python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s","python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p \"You're my trusty AI sidekick\" -cnv","This command will put the model into chat mode,","and use \"You're my trusty AI sidekick\" as the opening prompt."]},"click1":{"zh_CN":["你可以在命令后面加一些参数来调整：","-n: 控制模型生成多少个词。","-t: 用多少个 CPU 线程来跑。","-temp: 控制AI回复的“脑洞”大小。"],"en":["You can add some parameters after the command to adjust:","-n: Controls how many tokens the model generates.","-t: Specifies how many CPU threads to use.","-temp: Controls the 'creativity' or randomness of the AI's response."]},"click2":{"zh_CN":["如果你跑成功了，可能会看到类似这样的输出：","Yo, I’m your AI sidekick! Ready to answer questions, crack jokes, or maybe write a poem? What’s up?","感觉就像真的有个机器人助手坐在你电脑里跟你打招呼！","而且这一切都是你的CPU在默默完成。"],"en":["If you run it successfully, you might see output like this:","Yo, I’m your AI sidekick! Ready to answer questions, crack jokes, or maybe write a poem? What’s up?","It feels like you really have a robot assistant sitting in your computer greeting you!","And all of this is being quietly done by your CPU."]}}
---

## Step 5: 运行推理 (AI对话!)

设置环境后，运行 `run_inference.py`:

```bash
# 示例：以聊天模式启动
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You're my trusty AI sidekick" -cnv
```

<div v-click="1">

**常用参数:**
- `-n`: 生成tokens数量
- `-t`: 使用CPU线程数
- `-temp`: 控制回复随机性 (如 0.7)

</div>

<div v-click="2">

**示例输出:**
```text
Yo, I’m your AI sidekick! Ready to answer questions, crack jokes, or maybe write a poem? What’s up?
```

</div>

::right::



---
page: 12

layout: image-right
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["第六步：测测速度多快","如果你好奇自己的机器能跑多快，","可以跑个基准测试：","python utils/e2e_benchmark.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -n 200 -p 256 -t 4","这个命令会用256词的提示语，","让模型生成200个词，并用4个CPU线程。","跑完之后，它会告诉你速度和能耗等信息。","我跑的时候，看到我的旧笔记本","竟然能比我打字还快地输出文本，真是挺震撼的！"],"en":["Step 6: Test the Speed","If you're curious how fast your machine can run,","you can run a benchmark test:","python utils/e2e_benchmark.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -n 200 -p 256 -t 4","This command will use a 256-token prompt,","ask the model to generate 200 tokens, and use 4 CPU threads.","After running, it will tell you information like speed and energy consumption.","When I ran it, seeing my old laptop","outputting text faster than I could type was truly impressive!"]}}
---

## Step 6: 性能基准测试

想知道你的机器有多快？运行基准测试脚本：

```bash
python utils/e2e_benchmark.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -n 200 -p 256 -t 4
```

*使用256词提示，生成200词，用4线程。显示速度、能耗等统计信息*

---
page: 13

layout: two-cols
subtitles: {"default":{"zh_CN":["小彩蛋：玩玩“假”模型","如果你只想测试框架，不想下载大模型，","还可以创建一个假的（dummy）模型来玩：","python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M"],"en":["Little Bonus: Play with a 'Dummy' Model","If you just want to test the framework and don't want to download a large model,","you can also create a dummy model to play with:","python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M"]},"click1":{"zh_CN":["这会生成一个1.25亿参数的假模型，","然后你可以用它来跑基准测试。","python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128","这个功能很适合快速验证某个参数设置或测试框架流程。"],"en":["This will generate a dummy model with 125 million parameters,","and then you can use it to run benchmark tests.","python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128","This feature is great for quickly verifying a certain parameter setting or testing the framework process."]}}
---

## 💡 彩蛋: 玩转虚拟模型 (Dummy Model)

不下载完整大模型，也能测试框架：

```bash
# 生成一个1.25亿参数的虚拟模型
python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M
```


::right::


<div v-click="1">

# 对虚拟模型进行基准测试

```bash
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
```

*适合快速验证配置和框架流程*

</div>

---
page: 14

layout: two-cols
subtitles: {"default":{"zh_CN":["总的来说，BitNet.cpp 这个项目，"],"en":["In summary, the BitNet.cpp project,"]},"click1":{"zh_CN":["通过 1.58-bit 量化和底层优化，"],"en":["through 1.58-bit quantization and low-level optimization,"]},"click2":{"zh_CN":["把之前只能在高端硬件上运行的大型AI模型，","“搬”到了我们普通人手里的电脑上。"],"en":["has 'moved' large AI models that previously could only run on high-end hardware,","onto the computers in the hands of ordinary people."]},"click3":{"zh_CN":["这不仅仅是技术上的突破，","它打破了硬件壁垒，降低了AI应用的门槛。"],"en":["This is not just a technical breakthrough,","it breaks down hardware barriers and lowers the threshold for AI applications."]},"click4":{"zh_CN":["推动了AI的普及与创新"],"en":["promoting the popularization and innovation of AI"]},"click5":{"zh_CN":["让强大的AI能力变得触手可及，","也为AI的普及和创新打开了新的大门。"],"en":["making powerful AI capabilities within reach,","and opening up new doors for AI's widespread adoption and innovation."]}}
---

# 总结：AI民主化进程

<div v-click="1">

- BitNet.cpp 通过 1.58-bit 量化和底层优化

</div>

<div v-click="2">

- 将大型AI模型“搬”到普通PC的CPU上

</div>

<div v-click="3">

- 打破硬件壁垒，降低AI应用门槛

</div>

<div v-click="4">

- 推动AI普及与创新

</div>

<div v-click="5">

**强大AI能力，触手可及！**

</div>

::right::



---
page: 15

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["好啦，关于微软的 BitNet.cpp","和如何在你的笔记本上跑大型AI模型，","我就分享到这里。","那么，如果现在你的笔记本就能跑一个1000亿参数的AI模型，","你会用它来做点什么呢？"],"en":["Alright, that concludes my sharing about Microsoft's BitNet.cpp","and how to run large AI models on your laptop.","So, if your laptop could now run a 100 billion parameter AI model,","what would you use it for?"]},"click1":{"zh_CN":["感谢你的收听！","如果你尝试了或者有什么想法，欢迎留言告诉我！"],"en":["Thank you for listening!","If you've tried it or have any thoughts, feel free to leave a comment and let me know!"]}}
---

# 交流与分享

如果你的笔记本就能跑一个1000亿参数的AI模型，
你会用它来做点什么呢？

<div v-click="1">

**感谢收听！**
**欢迎留言分享你的想法或提问！**

</div>
---
page: 1

theme: seriph
background: https://cover.sli.dev
title: "null"
titleTemplate: '%s - Slaide'
layout: cover
presenter: dev
seoMeta:
  ogTitle: "null"
addons:
  - slidev-theme-viplay
subtitlesConfig:
  noTTSDelay: 2000
  ttsApi: "https://edgetts.deno.dev/v1/audio/speech"
  ttsLangName:
    en: "English(US)"
    zh_CN: "中文(简体)"
  apiCustom:
    voice: 'rate:-0.2|pitch:0.1'
  ttsModel:
    zh_CN:
      - value: "zh-CN-YunjianNeural"
        display: "云间"
      - value: "zh-CN-XiaoxiaoNeural"
        display: "晓晓"
    en:
      - value: "en-US-AndrewNeural"
        display: "Andrew"
      - value: "en-US-AriaNeural"
        display: "Aria"
subtitles: {"default":{"zh_CN":["哈喽！欢迎回来！","今天咱们来聊一个超酷的话题，","是大模型领域这两年特别火、也特别关键的一件事","——怎么把大模型做得又大又快，","还不至于贵到只有少数人玩得起。"],"en":["Hello everyone, welcome back!","Today we're going to talk about a super cool topic,","Something that's been particularly hot and crucial in the large model field in the past two years","– how to make large models bigger and faster,","without being so expensive that only a few can afford them."]}}
---

# DeepSeek-V3方法论深度解析

## 大规模语言模型扩展与硬件协同设计的创新实践

**报告人：[你的名字]**

---
page: 2

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["嗯，今天要深挖的就是 DeepSeek-V3 这个模型，","它在解决大模型扩展难题上，"],"en":["Well, today we're going to deep dive into the DeepSeek-V3 model,","which addresses the challenges of large model scaling."]},"click1":{"zh_CN":["提出了整套非常有意思的","“硬件感知模型协同设计”方法。"],"en":["It proposes a whole set of very interesting","“Hardware-Aware Model Co-design” methods."]},"click2":{"zh_CN":["它的目标是","实现大规模场景下经济高效的训练和推理。"],"en":["Its goal is to achieve","cost-efficient training and inference in large-scale scenarios."]},"click3":{"zh_CN":["通过集成各种创新技术，","深度融合模型与硬件特性。"],"en":["By integrating various innovative technologies,","it deeply integrates model and hardware characteristics."]}}
---

## DeepSeek-V3: 硬件感知模型协同设计

- **DeepSeek-V3**: 应对LLM扩展挑战的模型

<div v-click="1">

- **核心策略**: 硬件感知模型协同设计 (Hardware-Aware Model Co-design)

</div>

<div v-click="2">

- **目标**: 实现大规模场景下**经济高效**的训练和推理

</div>

<div v-click="3">

- **方法**: 集成创新技术，深度融合模型与硬件特性

</div>

---
page: 3

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["引言：大模型扩展的“三重困境”","你知道吗，虽然大模型能力越来越强，","但要把它做得更大、训练得更快、用起来更方便，","可不是一件容易的事儿。"],"en":["Introduction: The 'Triple Dilemma' of Large Language Model Scaling","Did you know, although large language models are becoming more powerful,","making them bigger, training them faster, and using them more conveniently,","is not an easy task."]},"click1":{"zh_CN":["大模型在飞速发展，能力越来越强。","但是..."],"en":["LLMs are developing rapidly, becoming more and more powerful.","However..."]},"click2":{"zh_CN":["它的扩展面临严峻的硬件挑战。"],"en":["Its scaling faces severe hardware challenges."]},"click3":{"zh_CN":["这限制了它的普及与发展。"],"en":["This restricts its popularization and development."]}}
---

## 引言：高效LLM扩展的迫切性

### 大规模语言模型扩展的“三重困境”

<div v-click="1">

- LLM飞速发展，能力越来越强

</div>

<div v-click="2">

- 扩展面临严峻硬件挑战

</div>

<div v-click="3">

- 普及与发展受限

</div>

---
page: 4

layout: two-cols
subtitles: {"default":{"zh_CN":["我看到，这背后其实有个“三重困境”在卡脖子：/D/1000"],"en":["I see, behind this, there's actually a 'triple dilemma' holding things back:/D/1000"]},"click1":{"zh_CN":["1. 内存墙：/D/500","想象一下，模型参数动辄千亿、万亿，","这需要占用海量内存。","尤其是在推理的时候，","那个叫 KV Cache（键值缓存）的东西，","会随着你输入的文本越来越长，","像滚雪球一样膨胀。","它占用的内存，","有时候比模型参数本身都多！","内存容量不够、带宽不够快，","数据送不过来，计算单元就等着，急死人了。"],"en":["1. Memory Wall:/D/500","Imagine, model parameters easily reach hundreds of billions or trillions,","This requires consuming massive amounts of memory.","Especially during inference,","That thing called KV Cache (Key-Value Cache),","Will expand like a snowball as your input text gets longer and longer.","The memory it occupies,","Sometimes even more than the model parameters themselves!","If memory capacity is insufficient or bandwidth is not fast enough,","Data cannot be delivered, and the computation units just wait, which is frustrating."]},"click2":{"zh_CN":["2. 计算成本：","训练和跑一个万亿参数的模型，需要的计算量大得吓人，","每处理一个词（token），都涉及海量的矩阵乘法。","这可是实打实的计算资源消耗，","非常昂贵。"],"en":["2. Computational Cost:","Training and running a trillion-parameter model requires an intimidating amount of computation,","Every word (token) processed involves massive matrix multiplications.","This is a real consumption of computational resources,","Very expensive."]},"click3":{"zh_CN":["3. 通信开销：","训练这么大的模型，","得动用成百上千甚至更多的计算卡（比如GPU），","这些卡之间得互相“说话”，","比如同步梯度、分发参数、传递中间结果。","这个“说话”的过程，也就是数据通信，","如果网络不够快、延迟高，","就会成为整个系统的瓶颈。"],"en":["3. Communication Overhead:","Training such a large model,","Requires hundreds or even thousands of computing cards (like GPUs),","These cards must 'talk' to each other,","For example, synchronizing gradients, distributing parameters, passing intermediate results.","This 'talking' process, which is data communication,","If the network is not fast enough or latency is high,","It will become the bottleneck of the entire system."]}}
---

## 大规模语言模型扩展的“三重困境”

<div v-click="1">

1.  **内存墙 (Memory Wall)**
    -   KV Cache 随序列长度急剧膨胀
    -   模型参数庞大
    -   内存带宽限制

</div>

<div v-click="2">

2.  **计算成本 (Computational Cost)**
    -   数万亿参数模型需海量计算
    -   每个token处理涉及大量矩阵运算

</div>

<div v-click="3">

3.  **通信开销 (Communication Overhead)**
    -   分布式训练/推理中加速器间数据通信
    -   模型并行下延迟与带宽限制

</div>

::right::



---
page: 5

layout: two-cols
subtitles: {"default":{"zh_CN":["这些问题加起来，","就像是建摩天大楼遇到的","地基不稳、钢筋不够、塔吊太慢一样，","严重限制了LLM的普及和发展。","所以，克服这些瓶颈对于推动LLM技术的发展至关重要。"],"en":["These problems combined,","Are like encountering issues when building a skyscraper,","An unstable foundation, insufficient rebar, or a crane that's too slow,","Severely limiting the popularization and development of LLMs.","Therefore, overcoming these bottlenecks is crucial for driving the development of LLM technology."]},"click1":{"zh_CN":["它推高了开发和部署成本。"],"en":["It drives up development and deployment costs."]},"click2":{"zh_CN":["限制了它在资源受限环境中的应用。"],"en":["It restricts its application in resource-constrained environments."]},"click3":{"zh_CN":["构成了经济和普适性上的壁垒。"],"en":["Forming economic and universality barriers."]},"click4":{"zh_CN":["DeepSeek-V3着重于“经济高效的训练和推理”，","这表明其战略目标在于推动SOTA LLM能力","超越少数超大规模计算机构，","向更广泛的开发者和研究者普及。"],"en":["DeepSeek-V3 focuses on 'cost-efficient training and inference',","indicating its strategic goal is to push SOTA LLM capabilities","beyond a few ultra-large-scale computing institutions,","to popularize it among a wider range of developers and researchers."]}}
---

## “三重困境”的深层影响

<div v-click="1">

- 推高开发和部署成本

</div>

<div v-click="2">

- 限制资源受限环境应用

</div>

<div v-click="3">

- 构成经济和普适性壁垒

</div>

<div v-click="4">

DeepSeek-V3 目标：**经济高效的训练和推理**，普及SOTA LLM能力。

</div>

::right::



---
page: 6

layout: two-cols
subtitles: {"default":{"zh_CN":["那DeepSeek-V3是怎么破局的呢？"],"en":["So how does DeepSeek-V3 break through?"]},"click1":{"zh_CN":["它的核心理念就是","“硬件感知模型协同设计”。","这听起来有点学究气，","但说白了就是，设计模型的时候，","不是拍脑袋想一个架构，","然后再想怎么往硬件上塞。"],"en":["Its core concept is","“Hardware-Aware Model Co-design”.","This sounds a bit academic,","But put simply, when designing the model,","You don't just come up with an architecture off the top of your head,","And then figure out how to squeeze it onto the hardware."]},"click2":{"zh_CN":["传统的模式是，先设计一个模型，","然后才考虑怎么在硬件上部署。"],"en":["The traditional mode is to design a model first,","And only then consider how to deploy it on hardware."]},"click3":{"zh_CN":["DeepSeek的理念是，","从一开始就得盯着硬件的能力和限制来设计模型，","让模型架构和底层的硬件特性深度融合。","就像是量体裁衣，","根据现有硬件这块“布”，","设计最合身的“衣服”——模型。"],"en":["DeepSeek's philosophy is,","From the beginning, you must design the model by looking at the hardware's capabilities and limitations,","Letting the model architecture and underlying hardware characteristics deeply integrate.","It's like tailoring a suit,","Using the existing hardware as the 'fabric',","Designing the best-fitting 'clothing' – the model."]},"click4":{"zh_CN":["这是一个从孤立设计到集成式方法的范式转变。"],"en":["This is a paradigm shift from isolated design to an integrated approach."]},"click5":{"zh_CN":["这种方法带来很多优势：","优化性能、延迟、能效；","我感觉，这种思路特别务实，","能让一个相对没那么“土豪”的团队，","用有限的硬件资源，也能玩转超大模型。"],"en":["This approach brings many advantages:","Optimizing performance, latency, and energy efficiency;","I feel this approach is particularly pragmatic,","Allowing a team that isn't particularly 'rich' in resources,","To also handle large models with limited hardware resources."]}}
---

## 核心理念：硬件感知模型协同设计

<div v-click="1">

它的核心理念就是“硬件感知模型协同设计”。

</div>

<div v-click="2">

- **传统模式**: 先设计模型，再考虑硬件部署。

</div>

<div v-click="3">

- **DeepSeek理念**: 从一开始就将**硬件约束和能力**纳入模型设计。

</div>

<div v-click="4">

- **范式转变**: 从孤立设计到**集成式方法**。

</div>

<div v-click="5">

- **优势**: 优化性能、延迟、能效；支持较小团队训练大型模型。

</div>

::right::



---
page: 7

layout: two-cols
subtitles: {"default":{"zh_CN":["DeepSeek-V3的创新武器库","为了实现这个目标，","DeepSeek-V3拿出了好几件“武器”：/D/1000"],"en":["DeepSeek-V3's Innovation Toolkit","To achieve this goal,","DeepSeek-V3 brought out several 'weapons':/D/1000"]},"click1":{"zh_CN":["本报告后续章节将详细阐述表1中列出的各项关键技术方法。","如幻灯片上图表所示，DeepSeek-V3采用了一系列创新方法来应对扩展挑战。","下表概述了其中关键的技术及其目标和核心机制。"],"en":["Subsequent chapters of this report will elaborate on the key technical methods listed in Table 1.","As shown in the table on the slide, DeepSeek-V3 adopts a series of innovative methods to address scaling challenges.","The table below outlines the key technologies, their objectives, and core mechanisms."]},"click2":{"zh_CN":["- 第2节将深入探讨多头隐注意力（MLA）如何提高内存效率。","- 第3节将分析混合专家（MoE）架构和多Token预测（MTP）在优化成本和计算方面的作用。","- 第4节将介绍以低精度为驱动的设计，包括FP8训练和LogFMT通信压缩。","- 第5节将讨论互连和网络驱动的设计，重点是节点限制路由和多平面胖树网络。","- 第6节将综合讨论硬件-模型协同设计的实践，并展望未来的硬件发展方向。","- 最后，第7节将对DeepSeek-V3的贡献进行总结。"],"en":["- Section 2 will delve into how Multi-Head Latent Attention (MLA) improves memory efficiency.","- Section 3 will analyze the role of Mixture-of-Experts (MoE) architecture and Multi-Token Prediction (MTP) in optimizing cost and computation.","- Section 4 will introduce low-precision driven designs, including FP8 training and LogFMT communication compression.","- Section 5 will discuss interconnect and network-driven designs, focusing on Node-Limited Routing and Multi-Plane Fat-Tree networks.","- Section 6 will comprehensively discuss the practice of hardware-model co-design and look ahead to future hardware development directions.","- Finally, Section 7 will summarize DeepSeek-V3's contributions."]}}
---

## DeepSeek-V3关键方法论概览

如幻灯片上图表所示，DeepSeek-V3采用了一系列创新方法：

| 方法论                  | 主要目标        | 关键创新/机制                      | 解决的挑战                    |
| :---------------------- | :-------------- | :--------------------------------- | :---------------------------- |
| 多头隐注意力 (MLA)      | 减少KV缓存      | 隐式KV压缩与上投影               | 内存墙                        |
| 混合专家 (MoE) 架构     | 优化计算/通信   | 细粒度专家，无辅助损失负载均衡   | 高计算成本                    |
| FP8混合精度训练         | 减少内存/加速   | 激活/权重分块量化                | 内存带宽，计算效率            |
| LogFMT通信压缩          | 减少通信量      | 专家并行FP8量化通信              | 通信瓶颈                      |
| 节点限制路由            | 最小化跨节点流  | Token路由限制最多4节点           | MoE跨节点通信开销             |
| 多平面胖树 (MPFT) 网络  | 经济高效横向扩展 | GPU-NIC对分配独立网络平面        | 可扩展网络基础设施            |

<div v-click="1">

本报告后续章节将详细阐述表1中列出的各项关键技术方法。
如幻灯片上图表所示，DeepSeek-V3采用了一系列创新方法来应对扩展挑战。
下表概述了其中关键的技术及其目标和核心机制。

</div>

<div v-click="2">

- 第2节将深入探讨多头隐注意力（MLA）如何提高内存效率。
- 第3节将分析混合专家（MoE）架构和多Token预测（MTP）在优化成本和计算方面的作用。
- 第4节将介绍以低精度为驱动的设计，包括FP8训练和LogFMT通信压缩。
- 第5节将讨论互连和网络驱动的设计，重点是节点限制路由和多平面胖树网络。
- 第6节将综合讨论硬件-模型协同设计的实践，并展望未来的硬件发展方向。
- 最后，第7节将对DeepSeek-V3的贡献进行总结。

</div>

::right::



---
page: 8

layout: two-cols
subtitles: {"default":{"zh_CN":["1. 多头隐注意力 (MLA)：","内存的魔术师/D/1000"],"en":["1. Multi-Head Latent Attention (MLA):","The Memory Magician/D/1000"]},"click1":{"zh_CN":["前面说了，KV Cache是个内存大户。","在标准的Transformer架构中，自注意力机制（Self-Attention）是核心组件。","然而，其一大弊端在于键值缓存（KV Cache）的内存消耗。","在推理过程中，为了避免重复计算，每个先前token的键（Key）和值（Value）向量都会被缓存起来。","随着输入序列长度的增加，KV缓存的大小呈线性增长，对于长序列任务，这会消耗巨大的内存资源。","例如，标准的多头注意力（MHA）机制为每个注意力头都维护独立的K和V，导致KV缓存迅速膨胀。","即使是后续提出的分组查询注意力（GQA）和多查询注意力（MQA）等变体，虽然通过共享K和V头减少了缓存大小，但仍面临内存压力，并可能以牺牲模型表达能力为代价。","KV缓存的瓶颈直接限制了模型能够处理的上下文长度，并增加了推理成本。"],"en":["As mentioned before, KV Cache is a major memory consumer.","In the standard Transformer architecture, Self-Attention is a core component.","However, a major drawback is the memory consumption of the Key-Value Cache (KV Cache).","During inference, to avoid re-computation, the key and value vectors for each previous token are cached.","As the input sequence length increases, the size of the KV cache grows linearly, consuming significant memory resources for long sequence tasks.","For example, the standard Multi-Head Attention (MHA) mechanism maintains independent K and V for each attention head, leading to rapid KV cache expansion.","Even subsequent variants like Grouped-Query Attention (GQA) and Multi-Query Attention (MQA), while reducing cache size by sharing K and V heads, still face memory pressure and may do so at the cost of model expressiveness.","The KV cache bottleneck directly limits the context length a model can handle and increases inference cost."]}}
---

## 核心架构创新：多头隐注意力 (MLA)

### 实现内存效率

- **KV缓存瓶颈**: 标准注意力(MHA, GQA)中KV Cache随序列增长线性膨胀，消耗大量内存。

<div v-click="1">

前面说了，KV Cache是个内存大户。
在标准的Transformer架构中，自注意力机制（Self-Attention）是核心组件。
然而，其一大弊端在于键值缓存（KV Cache）的内存消耗。
在推理过程中，为了避免重复计算，每个先前token的键（Key）和值（Value）向量都会被缓存起来。
随着输入序列长度的增加，KV缓存的大小呈线性增长，对于长序列任务，这会消耗巨大的内存资源 3。
例如，标准的多头注意力（MHA）机制为每个注意力头都维护独立的K和V，导致KV缓存迅速膨胀。
即使是后续提出的分组查询注意力（GQA）和多查询注意力（MQA）等变体，虽然通过共享K和V头减少了缓存大小，但仍面临内存压力，并可能以牺牲模型表达能力为代价 4。
KV缓存的瓶颈直接限制了模型能够处理的上下文长度，并增加了推理成本。

</div>

::right::



---
page: 9

layout: two-cols
subtitles: {"default":{"zh_CN":["为了解决KV缓存瓶颈，DeepSeek引入了多头隐注意力（MLA）机制。","MLA的核心思想是将所有注意力头的键值（KV）表示压缩到一个共享的、更小的隐向量（latent vector）中，从而显著减少KV缓存的大小。","MLA的具体机制包括："],"en":["To address the KV cache bottleneck, DeepSeek introduced the Multi-Head Latent Attention (MLA) mechanism.","The core idea of MLA is to compress the Key-Value (KV) representation of all attention heads into a shared, smaller latent vector, thereby significantly reducing the size of the KV cache.","MLA's specific mechanisms include:"]},"click1":{"zh_CN":["低秩键值联合压缩 (Low-Rank Key-Value Joint Compression)：MLA在键值层使用低秩矩阵。","它将原始的投影矩阵 W 分解为一个降维投影矩阵 WDKV​∈Rdc​×d 和一个升维投影矩阵 WU​∈Rdh​nh​×dc​，","其中 d 是输入嵌入的维度，dh​ 是每个头的维度，nh​ 是头的数量，dc​ 是隐向量的维度，且 dc​≪dh​nh​。","降维投影矩阵 WDKV​ 将每个token ht​ 的键和值压缩成一个共享的隐向量 cKVt​=WDKV​ht​，其中 cKVt​∈Rdc​。","由于 dc​ 远小于 2dh​nh​，存储 cKVt​ 代替存储完整的 kt​ 和 vt​ 可以大幅减少KV缓存。"],"en":["Low-Rank Key-Value Joint Compression: MLA uses low-rank matrices in the key-value layer.","It decomposes the original projection matrix W into a dimensionality reduction projection matrix WDKV​∈Rdc​×d and an up-projection matrix WU​∈Rdh​nh​×dc​,","where d is the dimension of the input embedding, dh​ is the dimension of each head, nh​ is the number of heads, dc​ is the dimension of the latent vector, and dc​≪dh​nh​.","The dimensionality reduction projection matrix WDKV​ compresses the key and value of each token ht​ into a shared latent vector cKVt​=WDKV​ht​, where cKVt​∈Rdc​.","Since dc​ is much smaller than 2dh​nh​, storing cKVt​ instead of storing the full kt​ and vt​ can significantly reduce KV cache."]},"click2":{"zh_CN":["上投影矩阵 (Up-Projection Matrix)：","虽然KV被压缩，但MLA通过上投影矩阵（例如用于键的 WUK​ 和用于值的 WUV​）来恢复或增强模型的表达能力。","这实际上是用额外的计算（上投影操作）换取了显著的内存和通信开销的降低。"],"en":["Up-Projection Matrix:","Although KV is compressed, MLA restores or enhances the model's expressiveness through up-projection matrices (e.g., WUK​ for keys and WUV​ for values).","This effectively trades additional computation (up-projection operations) for significant reductions in memory and communication overhead."]},"click3":{"zh_CN":["推理效率 (Inference Efficiency)：","在推理时，键的上投影矩阵 WUK​ 可以被吸收到查询投影矩阵 WQ​ 中，","值的上投影矩阵 WUV​ 可以被吸收到输出投影矩阵 WO​ 中。","这意味着不需要显式地从隐向量 cKVt​ 计算出完整的键 ktC​ 和值 vtC​，进一步提高了推理效率。"],"en":["Inference Efficiency:","During inference, the up-projection matrix WUK​ for keys can be absorbed into the query projection matrix WQ​,","and the up-projection matrix WUV​ for values can be absorbed into the output projection matrix WO​.","This means there is no need to explicitly compute the full key ktC​ and value vtC​ from the latent vector cKVt​, further improving inference efficiency."]},"click4":{"zh_CN":["旋转位置编码的解耦 (Decoupling RoPE)：","为了高效地将旋转位置编码（RoPE）与MLA结合，DeepSeek-V2（DeepSeek-V3可能继承或发展了此设计）提出将RoPE解耦为一组独立的查询和键。","模型计算两组独立的注意力权重，然后将它们相加。","这种策略避免了因RoPE导致 WUK​ 无法被吸收到 WQ​ 中而带来的显著计算成本。"],"en":["Decoupling RoPE:","To efficiently combine Rotational Positional Encoding (RoPE) with MLA, DeepSeek-V2 (which DeepSeek-V3 may inherit or evolve from) proposed decoupling RoPE into a set of independent queries and keys.","The model computes two sets of independent attention weights and then sums them.","This strategy avoids the significant computational cost incurred by RoPE preventing WUK​ from being absorbed into WQ​."]},"click5":{"zh_CN":["MLA带来的主要优势包括：","与MHA/GQA相比，显著减小KV缓存大小。","由于内存占用减小，推理速度得到提升。","在减少KV缓存的同时，据称能达到甚至超越MHA的性能。","有效解决了LLM扩展中的核心挑战之一——内存效率问题。"],"en":["The main advantages brought by MLA include:","Significantly reducing KV cache size compared to MHA/GQA.","Improved inference speed due to reduced memory footprint.","Reportedly achieving performance comparable to or even surpassing MHA while reducing KV cache.","Effectively addressing one of the core challenges in LLM scaling – memory efficiency."]}}
---

## MLA机制与优势

<div v-click="1">

- **核心思想**: 将所有注意力头的KV压缩到共享的、更小的**隐向量**。
- **低秩KV压缩**:
    -   将权重矩阵 $W$ 分解为降维 $W_{DKV} \in \mathbb{R}^{d_c \times d}$ 和升维 $W_U \in \mathbb{R}^{d_h n_h \times d_c}$
    -   隐向量 $c_{KV_t} = W_{DKV} h_t$, $d_c \ll d_h n_h$

</div>

<div v-click="2">

- **上投影矩阵 ($W_U$)**: 恢复/增强表达能力，计算换空间。

</div>

<div v-click="3">

- **推理效率**: 上投影矩阵可融入现有计算。

</div>

<div v-click="4">

- **RoPE解耦**: 高效结合旋转位置编码。

</div>

<div v-click="5">

- **优势**: 显著减小KV缓存，提升推理速度，据称性能比肩MHA。

</div>

::right::



---
page: 10

layout: two-cols
subtitles: {"default":{"zh_CN":["MLA，特别是其上投影矩阵的设计，体现了一种深思熟虑的权衡：","通过增加一定的计算操作，换取内存占用和通信带宽的大幅降低。"],"en":["MLA, particularly the design of its up-projection matrix, embodies a deliberate trade-off:","By adding certain computational operations, it exchanges this for a significant reduction in memory usage and communication bandwidth."]},"click1":{"zh_CN":["这种权衡之所以可行且有效，前提是底层硬件（如DeepSeek-V3使用的NVIDIA H800 GPU）能够高效处理这些额外的计算，","而内存带宽则被视为更严重的瓶颈。"],"en":["This trade-off is feasible and effective because the underlying hardware (such as the NVIDIA H800 GPUs used by DeepSeek-V3) can efficiently handle these additional computations,","while memory bandwidth is considered a more severe bottleneck."]},"click2":{"zh_CN":["MLA的优势就在于，它识别到","内存带宽往往是比计算更紧迫的瓶颈。"],"en":["MLA's advantage lies in its recognition that","memory bandwidth is often a more pressing bottleneck than computation."]},"click3":{"zh_CN":["这正是硬件感知模型协同设计的一个直接体现。","模型的设计与硬件的能力紧密结合。"],"en":["This is a direct manifestation of hardware-aware model co-design.","The model's design is closely integrated with the hardware's capabilities."]}}
---

## MLA：计算与内存的权衡

<div v-click="1">

- **深思熟虑的权衡**: 增加少量计算换取内存和通信大幅降低。

</div>

<div v-click="2">

- **硬件前提**: 底层硬件（如H800）能高效处理额外计算。

</div>

<div v-click="3">

- ** MLA优势**: 内存带宽是更严重瓶颈。

</div>

<div v-click="4">

- **硬件感知体现**: MLA设计与硬件能力紧密结合。

</div>

::right::



---
page: 11

layout: two-cols
subtitles: {"default":{"zh_CN":["2.3. 与其他注意力变体的比较 (MHA, GQA)/D/1000"],"en":["2.3. Comparison with other Attention Variants (MHA, GQA)/D/1000"]},"click1":{"zh_CN":["与传统的MHA和GQA相比，MLA提供了不同的权衡。","GQA可以通过将多个查询头分组并共享同一组键和值头来减少KV缓存，","但MLA声称可以在相同的KV缓存开销下表示GQA，而反之则不然。","这意味着MLA可能具有更大的表达潜力。","此外，GQA和MQA等方法通过减少注意力机制中的参数数量来缩小KV缓存，","这有时会导致性能下降，","而MLA则试图通过上投影机制在压缩KV的同时保持甚至增强模型的表达能力。"],"en":["Compared to traditional MHA and GQA, MLA offers different trade-offs.","GQA can reduce KV cache by grouping multiple query heads and sharing the same set of key and value heads,","but MLA claims it can represent GQA with the same KV cache overhead, while the reverse is not true.","This means MLA may have greater expressive potential.","Furthermore, methods like GQA and MQA reduce KV cache size by reducing the number of parameters in the attention mechanism,","which can sometimes lead to performance degradation,","whereas MLA attempts to maintain or even enhance the model's expressive power while compressing KV through the up-projection mechanism."]},"click2":{"zh_CN":["如幻灯片上图表所示，这是MLA与其他注意力变体的比较：/D/1000"],"en":["As shown in the table on the slide, here is a comparison of MLA with other attention variants:/D/1000"]},"click3":{"zh_CN":["尽管MLA展现出显著优势，","但其在其他主流模型提供商中的采用速度相对较慢。","例如，TransMLA这类工具的出现，","旨在将基于GQA的预训练模型转换为基于MLA的模型，","这既说明了推广MLA的潜力，","也反映了从GQA等成熟方法迁移的惯性或架构层面的挑战","（例如，MHA和MLA在架构上的差异使得零样本迁移不切实际）。"],"en":["Despite MLA demonstrating significant advantages,","Its adoption rate among other mainstream model providers is relatively slow.","For example, the emergence of tools like TransMLA,","Aimed at converting GQA-based pre-trained models to MLA-based models,","Both illustrates the potential for promoting MLA,","And reflects the inertia of migrating from mature methods like GQA or architectural challenges","(e.g., the architectural differences between MHA and MLA make zero-shot migration impractical)."]},"click4":{"zh_CN":["DeepSeek在MLA上的成功实践，","可能成为推动其更广泛应用的催化剂。"],"en":["DeepSeek's successful practice with MLA,","May become a catalyst for its wider application."]}}
---

## 注意力机制对比分析

如幻灯片上图表所示，这是MLA与其他注意力变体的比较：

<div v-click="1">

与传统的MHA和GQA相比，MLA提供了不同的权衡。
GQA可以通过将多个查询头分组并共享同一组键和值头来减少KV缓存，但MLA声称可以在相同的KV缓存开销下表示GQA，而反之则不然 3。
这意味着MLA可能具有更大的表达潜力。
此外，GQA和MQA等方法通过减少注意力机制中的参数数量来缩小KV缓存，这有时会导致性能下降，而MLA则试图通过上投影机制在压缩KV的同时保持甚至增强模型的表达能力 4。

</div>

<div v-click="2">

| 特性               | 多头注意力 (MHA)   | 分组查询注意力 (GQA) | DeepSeek-V3 多头隐注意力 (MLA) |
| :----------------- | :----------------- | :--------------------- | :----------------------------- |
| **KV缓存大小**     | 大                 | 中等                   | 小                             |
| **计算开销**       | 高                 | 中等                   | 优化的权衡 (压缩+上投影)       |
| **模型表达能力**   | 高                 | 可能有所下降           | 通过上投影维持/增强            |
| **适用长上下文** | 内存受限           | 优于MHA                | 显著改进                       |

</div>

<div v-click="3">

如幻灯片上图表所示，这是MLA与其他注意力变体的比较：

</div>

<div v-click="4">

尽管MLA展现出显著优势，但其在其他主流模型提供商中的采用速度相对较慢 3。
例如，TransMLA这类工具的出现，旨在将基于GQA的预训练模型转换为基于MLA的模型 3，这既说明了推广MLA的潜力，也反映了从GQA等成熟方法迁移的惯性或架构层面的挑战（例如，MHA和MLA在架构上的差异使得零样本迁移不切实际 4）。

</div>

<div v-click="5">

DeepSeek在MLA上的成功实践，可能成为推动其更广泛应用的催化剂。

</div>

::right::



---
page: 12

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["2. 混合专家 (MoE) 架构：","算力按需分配/D/1000"],"en":["2. Mixture-of-Experts (MoE) Architecture:","Computing power on demand/D/1000"]},"click1":{"zh_CN":["MoE架构是一种通过在推理时为每个输入token动态选择并激活一部分“专家”网络（即模型参数子集）来降低计算成本的技术，","相较于激活所有参数的稠密模型，MoE能显著减少计算量。"],"en":["MoE architecture is a technique that reduces computational cost by dynamically selecting and activating a subset of 'expert' networks (i.e., model parameters) for each input token during inference,","Compared to dense models that activate all parameters, MoE can significantly reduce computation."]},"click2":{"zh_CN":["DeepSeek-V3采用了名为DeepSeekMoE的特定MoE实现：/D/1000"],"en":["DeepSeek-V3 adopts a specific MoE implementation called DeepSeekMoE:/D/1000"]},"click3":{"zh_CN":["参数规模与激活：DeepSeek-V3拥有高达6710亿的总参数量，","但在处理每个token时仅激活其中的370亿参数（约占5.5%）。","这种稀疏激活是MoE效率的核心。"],"en":["Parameter Scale and Activation: DeepSeek-V3 has a total parameter count of up to 671 billion,","But only activates 37 billion of these parameters (about 5.5%) when processing each token.","This sparse activation is the core of MoE's efficiency."]}}
---

## 优化成本与计算：混合专家 (MoE)

### 实现计算效率

- **MoE概念**: 推理时只激活部分“专家”网络 (参数子集)，降低计算成本。

<div v-click="1">

- **DeepSeekMoE**: 特定实现。

</div>

<div v-click="2">

- 总参数: 6710亿
- 每个token激活: 370亿 (约5.5%)

</div>

---
page: 13

layout: two-cols
subtitles: {"default":{"zh_CN":["它在MoE上做了些改进：/D/1000"],"en":["It made some improvements on MoE:/D/1000"]},"click1":{"zh_CN":["细粒度专家 (Fine-grained Experts)：","采用更细粒度的专家设计。","具体而言，专家数量从 N 增加到 mN，每个专家的隐藏维度减少为原来的 1/m，","并且每个token会激活 m 个更多的专家。","这种设计旨在促进知识在专家之间进行更充分的分解，同时保持总计算成本不变。"],"en":["Fine-grained Experts:","Adopts a more fine-grained expert design.","Specifically, the number of experts increases from N to mN, the hidden dimension of each expert is reduced to 1/m of the original,","And each token activates m more experts.","This design aims to promote a more thorough decomposition of knowledge among experts while keeping the total computational cost unchanged."]},"click2":{"zh_CN":["共享专家 (Shared Experts)：","隔离出一部分专家作为共享专家，用于学习跨任务的通用知识。","这使得其他专家能够更专注于特定领域的知识，从而提升模型的专业化能力和整体性能。"],"en":["Shared Experts:","Isolates a portion of experts as shared experts, used for learning general knowledge across tasks.","This allows other experts to focus more on domain-specific knowledge, thereby enhancing the model's specialization capability and overall performance."]},"click3":{"zh_CN":["路由组件 (Router Component)：","MoE架构中包含一个路由组件（通常是一个小型神经网络），","它负责根据输入token的特性，智能地将其导向最适合处理该token的一个或多个专家网络。"],"en":["Router Component:","The MoE architecture includes a router component (usually a small neural network),","which is responsible for intelligently directing each input token to one or more expert networks best suited to process it, based on the token's characteristics."]},"click4":{"zh_CN":["MoE架构带来的主要优势包括：","优化了计算与通信之间的权衡。","与同等能力的稠密模型相比，显著降低了每个token在训练和推理阶段的计算需求。","例如，尽管DeepSeek-V3的参数量是Qwen-72B的9倍，","但其每个token的计算量反而减少了36%。","提升了成本效益，尤其对于个人使用和本地化部署场景更具吸引力。"],"en":["The main advantages brought by the MoE architecture include:","Optimizing the trade-off between computation and communication.","Significantly reducing the computational requirements per token during training and inference compared to dense models of equivalent capability.","For example, although DeepSeek-V3 has 9 times the parameters of Qwen-72B,","Its per-token computation is reduced by 36% instead.","Improved cost-effectiveness, especially more attractive for personal use and localized deployment scenarios."]}}
---

## DeepSeekMoE 特点

<div v-click="1">

- **细粒度专家**: 更多、更小的专家，促进知识分解。

</div>

<div v-click="2">

- **共享专家**: 隔离部分专家学习通用知识。

</div>

<div v-click="3">

- **路由组件**: 根据token特性，导向相关专家。

</div>

<div v-click="4">

- **优势**: 优化计算/通信，降低每token计算量 (例: 比Qwen-72B少36%)，提升成本效益。

</div>

::right::



---
page: 14

layout: two-cols
subtitles: {"default":{"zh_CN":["无辅助损失的负载均衡 (Auxiliary-Loss-Free Load Balancing)：","这是DeepSeek-V3中一项新颖的MoE优化策略。"],"en":["Auxiliary-Loss-Free Load Balancing:","This is a novel MoE optimization strategy in DeepSeek-V3."]},"click1":{"zh_CN":["传统的MoE模型常常依赖辅助损失函数来平衡分配给各个专家的计算负载，","但这种辅助损失有时会损害模型性能。"],"en":["Traditional MoE models often rely on auxiliary loss functions to balance the computational load distributed among experts,","But this auxiliary loss can sometimes harm model performance."]},"click2":{"zh_CN":["DeepSeek-V3采用了一种无辅助损失的方法：","在计算top-K路由时的亲和度分数中添加一个偏置项。"],"en":["DeepSeek-V3 adopts an auxiliary-loss-free approach:","Adding a bias term to the affinity scores when computing top-K routing."]},"click3":{"zh_CN":["训练过程中会监控每个专家的负载情况，","如果某个专家过载或欠载，则相应调整其偏置项。","这种机制旨在避免路由崩溃和计算效率降低的问题，","而无需引入可能影响性能的辅助损失。","我觉得这个很实用，","解决了MoE实践中的一个痛点。"],"en":["During training, the load status of each expert is monitored,","If an expert is overloaded or underloaded, its bias term is adjusted accordingly.","This mechanism aims to avoid router collapse and reduced computational efficiency issues,","Without introducing auxiliary losses that might affect performance.","I find this very practical,","It solves a pain point in MoE practice."]},"click4":{"zh_CN":["它的优势在于，它能够避免路由崩溃，提高专家利用率，","同时不引入对模型性能的负面影响。"],"en":["Its advantage is that it can avoid router collapse, improve expert utilization,","While not introducing negative impacts on model performance."]}}
---

## DeepSeekMoE 负载均衡创新

<div v-click="1">

- **问题**: 传统MoE依赖辅助损失平衡专家负载，可能损害性能。

</div>

<div v-click="2">

- **DeepSeek方法**: **无辅助损失的负载均衡**。

</div>

<div v-click="3">

- 在top-K路由分数中添加**偏置项**。
- 训练中动态调整偏置项，响应专家过载/欠载状态。

</div>

<div v-click="4">

- **优势**: 避免路由崩溃，提高专家利用率，不引入性能负面影响。

</div>

::right::



---
page: 15

layout: two-cols
subtitles: {"default":{"zh_CN":["3. 多Token预测 (MTP)：","推理提速/D/1000"],"en":["3. Multi-Token Prediction (MTP):","Inference Acceleration/D/1000"]},"click1":{"zh_CN":["多Token预测（MTP）是DeepSeek-V3中采用的另一项旨在提升效率的技术。","其核心思想是在模型的每个预测步骤中，","不仅仅预测下一个token，而是同时预测未来多个token。"],"en":["Multi-Token Prediction (MTP) is another technique adopted in DeepSeek-V3 aimed at improving efficiency.","Its core idea is that in each prediction step of the model,","Instead of just predicting the next token, it predicts multiple future tokens simultaneously."]},"click2":{"zh_CN":["DeepSeek的MTP实现具有以下特点：","维持因果链 (Maintains Causal Chain)：","它通过顺序预测额外token的方式来维持语言模型固有的因果依赖关系。","这意味着对第 n+1 个token的预测会基于第 n 个token，","对第 n+2 个token的预测会基于第 n+1 个token，以此类推。"],"en":["DeepSeek's MTP implementation has the following characteristics:","Maintains Causal Chain:","It maintains the inherent causal dependencies of the language model by sequentially predicting additional tokens.","This means the prediction for the n+1th token is based on the nth token,","The prediction for the n+2th token is based on the n+1th token, and so on."]},"click3":{"zh_CN":["额外训练损失 (Additional Training Loss)：","在训练过程中，会引入一个额外的损失函数，","用于同时验证多个输出token的准确性。"],"en":["Additional Training Loss:","During training, an additional loss function is introduced,","Used to simultaneously verify the accuracy of multiple output tokens."]},"click4":{"zh_CN":["MTP带来的主要优势包括：","加速推理过程，提高token生成速率。","通过提供更密集的训练信号，提升训练效率和数据利用率。"],"en":["The main advantages brought by MTP include:","Accelerating the inference process and increasing the token generation rate.","Improving training efficiency and data utilization by providing more dense training signals."]}}
---

## 优化成本与计算：多Token预测 (MTP)

### 提升吞吐量

- **MTP核心**: 每个预测步骤同时预测未来**多个**token。

<div v-click="1">

- **DeepSeek MTP**: **顺序预测**额外token，维持因果链。

</div>

<div v-click="2">

- 训练: 引入额外损失验证多token准确性。

</div>

<div v-click="3">

- **优势**: 加速推理 (提高生成速率)，提升训练效率 (提供更密集的训练信号)。

</div>

::right::



---
page: 16

layout: two-cols
subtitles: {"default":{"zh_CN":["4. 低精度驱动设计：","数据减肥/D/1000"],"en":["4. Low-Precision Driven Design:","Data Diet/D/1000"]},"click1":{"zh_CN":["4.1. FP8混合精度训练","为了进一步降低内存消耗并加速计算，DeepSeek-V3采用了FP8混合精度训练技术。","动机：与广泛使用的BF16（16位浮点）格式相比，","FP8（8位浮点）可以将模型权重的内存占用减少一半，","有效缓解AI领域的“内存墙”问题。","同时，利用现代AI硬件对低精度计算的优化支持，FP8能够显著提升计算速度。"],"en":["4.1. FP8 Mixed-Precision Training","To further reduce memory consumption and accelerate computation, DeepSeek-V3 adopts FP8 mixed-precision training technology.","Motivation: Compared to the widely used BF16 (16-bit floating point) format,","FP8 (8-bit floating point) can reduce the memory footprint of model weights by half,","Effectively alleviating the 'memory wall' problem in the AI field.","At the same time, by utilizing the optimized support for low-precision computation in modern AI hardware, FP8 can significantly increase computation speed."]},"click2":{"zh_CN":["DeepSeek-V3实现：","DeepSeek-V3在其训练流程中明确使用了FP8混合精度。","具体而言，它采用了细粒度的量化方案：","对激活值进行tile-wise（例如1x128）量化，","对模型权重进行block-wise（例如128x128）量化。"],"en":["DeepSeek-V3 Implementation:","DeepSeek-V3 explicitly uses FP8 mixed precision in its training pipeline.","Specifically, it adopts a fine-grained quantization scheme:","quantizing activation values tile-wise (e.g., 1x128),","and quantizing model weights block-wise (e.g., 128x128)."]},"click3":{"zh_CN":["优势：","除了减少内存占用和加速计算外，","FP8还允许对权重、激活值乃至KV缓存进行量化，","从而实现更高效的推理。"],"en":["Advantages:","In addition to reducing memory usage and accelerating computation,","FP8 also allows for quantization of weights, activations, and even KV cache,","Thereby enabling more efficient inference."]}}
---

## 低精度驱动设计：数据“减肥”

### FP8混合精度训练

<div v-click="1">

- **动机**: 降低内存占用，加速计算。
    -   FP8 (8位) vs BF16 (16位): 内存减半。
    -   现代硬件优化FP8计算。

</div>

<div v-click="2">

- **DeepSeek实现**: **细粒度量化**。
    -   激活值: tile-wise (例: 1x128)
    -   权重: block-wise (例: 128x128)

</div>

<div v-click="3">

- **优势**: 减少内存，加速训练/推理。

</div>

::right::



---
page: 17

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["挑战与考量：","尽管优势明显，FP8的应用也面临挑战。","其较低的数值表示范围可能导致训练过程中的数值不稳定性，例如梯度下溢（vanishing gradients）。"],"en":["Challenges and Considerations:","Despite the obvious advantages, the application of FP8 also faces challenges.","Its lower numerical representation range may lead to numerical instability during training, such as vanishing gradients."]},"click1":{"zh_CN":["因此，成功实施FP8训练通常需要仔细的实现，可能包括调整超参数或使用动态缩放因子。","DeepSeek的论文提到其在FP8训练方面进行了“创新”，","暗示其已针对这些挑战开发了相应的解决方案。"],"en":["Therefore, successful implementation of FP8 training typically requires careful implementation, potentially including hyperparameter tuning or using dynamic scaling factors.","DeepSeek's paper mentions its 'innovation' in FP8 training,","Implying that it has developed corresponding solutions for these challenges."]},"click2":{"zh_CN":["FP8的采用不仅仅是一项软件层面的优化，","它与现代AI硬件（如DeepSeek-V3训练所用的NVIDIA H800 GPU）的能力紧密相关，","这些硬件通常内置了专门的计算单元以高效执行FP8运算。","DeepSeek-V3中“细粒度的量化”方案表明其采取了一种精细化的方法，","旨在平衡精度损失与性能增益，并充分利用目标硬件的特性。"],"en":["The adoption of FP8 is not just a software-level optimization,","It is closely related to the capabilities of modern AI hardware (such as the NVIDIA H800 GPUs used for DeepSeek-V3 training),","These hardware usually have dedicated computing units built-in to efficiently execute FP8 operations.","The 'fine-grained quantization' scheme in DeepSeek-V3 indicates that it adopts a refined approach,","Aimed at balancing accuracy loss with performance gains and fully leveraging the characteristics of the target hardware."]}}
---

## FP8训练：挑战与协同设计

<div v-click="1">

- **挑战**: 数值不稳定 (如梯度下溢)。

</div>

<div v-click="2">

- **DeepSeek**: 创新应对挑战，实现成功实施。

</div>

<div v-click="3">

- **协同设计体现**: FP8应用与现代AI硬件能力紧密相关 (硬件内置FP8计算单元)。细粒度量化平衡精度与性能。

</div>

---
page: 18

layout: two-cols
subtitles: {"default":{"zh_CN":["4.2. LogFMT：通信压缩","在分布式LLM训练和推理中，节点间的通信开销是一个主要瓶颈。"],"en":["4.2. LogFMT: Communication Compression","In distributed LLM training and inference, inter-node communication overhead is a major bottleneck."]},"click1":{"zh_CN":["LogFMT是DeepSeek-V3中用于缓解此问题的一种通信压缩技术。","概念：LogFMT的核心思想是使用低精度数据格式来压缩在加速器或节点之间传输的数据，","从而减少通信量和通信时间。"],"en":["LogFMT is a communication compression technique used in DeepSeek-V3 to alleviate this issue.","Concept: The core idea of LogFMT is to use low-precision data formats to compress data transmitted between accelerators or nodes,","Thereby reducing communication volume and communication time."]},"click2":{"zh_CN":["DeepSeek-V3实现：","DeepSeek-V3明确在其网络通信中采用了低精度压缩技术。","具体来说，在专家并行（Expert Parallelism, EP）场景下，","token（及其激活值）在分发给不同专家时，会使用细粒度的FP8量化进行压缩。"],"en":["DeepSeek-V3 Implementation:","DeepSeek-V3 explicitly uses low-precision compression technology in its network communication.","Specifically, in the Expert Parallelism (EP) scenario,","Tokens (and their activation values), when distributed to different experts, are compressed using fine-grained FP8 quantization."]},"click3":{"zh_CN":["与使用BF16格式相比，这可以将通信量减少50%。"],"en":["Compared to using the BF16 format, this can reduce communication volume by 50%."]},"click4":{"zh_CN":["论文还提到，","尽管“合并”（combine）阶段（可能是指聚合不同专家输出的阶段）由于精度要求可能仍使用较高精度（如BF16），","但团队也在积极测试FP8以及像E5M6这样的定制精度格式用于此阶段。"],"en":["The paper also mentions that,","Although the 'combine' stage (possibly referring to the stage where outputs from different experts are aggregated) may still use higher precision (like BF16) due to accuracy requirements,","The team is also actively testing FP8 and customized precision formats like E5M6 for this stage."]},"click5":{"zh_CN":["优势：","显著降低通信时间和通信数据量，","从而有效应对互连带宽的限制。"],"en":["Advantages:","Significantly reducing communication time and communication data volume,","Thereby effectively addressing interconnect bandwidth limitations."]},"click6":{"zh_CN":["LogFMT在专家并行中对token分发采用FP8压缩是一项具有战略意义的举措。","专家并行通常涉及将token及其激活值路由到位于不同节点上的专家。","对这些激活值进行压缩，能大幅减少在相对较慢的跨节点链路上移动的数据量，","直接缓解了分布式MoE模型中的一个关键通信瓶颈。"],"en":["Adopting FP8 compression for token distribution in Expert Parallelism within LogFMT is a strategically significant move.","Expert Parallelism typically involves routing tokens and their activation values to experts located on different nodes.","Compressing these activation values can significantly reduce the amount of data moved over relatively slower cross-node links,","Directly alleviating a key communication bottleneck in distributed MoE models."]},"click7":{"zh_CN":["同时，对“合并”阶段测试FP8表明团队在持续探索将低精度应用的边界进一步拓展，","并在效率和精度之间寻求最佳平衡点。"],"en":["At the same time, testing FP8 for the 'combine' stage indicates that the team is continuously exploring further expanding the boundaries of low-precision applications,","And seeking the optimal balance between efficiency and accuracy."]},"click8":{"zh_CN":["DeepSeek-V3并非采用一刀切的FP8策略，","而是在不同计算和通信阶段审慎地管理精度，","这种成熟的混合精度设计体现了其对精度要求的深刻理解。"],"en":["DeepSeek-V3 does not adopt a one-size-fits-all FP8 strategy,","But carefully manages precision in different computation and communication stages,","This mature mixed-precision design reflects its deep understanding of accuracy requirements."]}}
---

## 低精度驱动设计：数据“减肥”

### LogFMT通信压缩

- **概念**: 使用低精度数据格式压缩传输数据。

<div v-click="1">

- **DeepSeek实现**: 在**专家并行 (EP)** 场景下，对token/激活值使用**FP8量化**。

</div>

<div v-click="2">

- **效果**: 通信量减少 **50%** (对比BF16)。

</div>

<div v-click="3">

- **探索**: 积极测试FP8/定制精度用于**合并阶段**。

</div>

<div v-click="4">

- **优势**: 显著降低通信时间/数据量，应对互连带宽限制。

</div>

<div v-click="5">

LogFMT在专家并行中对token分发采用FP8压缩 19 是一项具有战略意义的举措。
专家并行通常涉及将token及其激活值路由到位于不同节点上的专家。
对这些激活值进行压缩，能大幅减少在相对较慢的跨节点链路上移动的数据量，直接缓解了分布式MoE模型中的一个关键通信瓶颈。

</div>

<div v-click="6">

同时，对“合并”阶段测试FP8 19 表明团队在持续探索将低精度应用的边界进一步拓展，并在效率和精度之间寻求最佳平衡点。

</div>

<div v-click="7">

DeepSeek-V3并非采用一刀切的FP8策略，而是在不同计算和通信阶段审慎地管理精度，这种成熟的混合精度设计体现了其对精度要求的深刻理解。

</div>

::right::



---
page: 19

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["5. 互连与网络驱动设计：","铺好数据高速路/D/1000"],"en":["5. Interconnect and Network-Driven Design:","Paving the Data Highway/D/1000"]},"click1":{"zh_CN":["现代大规模AI训练集群通常包含节点内高速互连（如NVLink）和节点间相对较慢的互连（如InfiniBand）。","这种带宽差异对并行策略的设计提出了挑战。","DeepSeek-V3通过硬件感知的并行策略和节点限制路由来应对这一问题。","带宽差异背景：节点内（scale-up）通信带宽（例如通过NVLink连接的GPU之间）远高于节点间（scale-out）通信带宽（例如通过InfiniBand连接的节点之间），","其比例可能达到4:1左右。"],"en":["Modern large-scale AI training clusters typically include high-speed intra-node interconnects (like NVLink) and relatively slower inter-node interconnects (like InfiniBand).","This bandwidth difference poses a challenge to the design of parallel strategies.","DeepSeek-V3 addresses this issue through hardware-aware parallel strategies and node-limited routing.","Bandwidth Difference Background: Intra-node (scale-up) communication bandwidth (e.g., between GPUs connected via NVLink) is significantly higher than inter-node (scale-out) communication bandwidth (e.g., between nodes connected via InfiniBand),","The ratio can be around 4:1."]},"click2":{"zh_CN":["它们的目标是规避网络瓶颈。"],"en":["Their goal is to circumvent network bottlenecks."]}}
---

## 互连与网络驱动设计

### 优化数据流

<div v-click="1">

- **挑战**: 节点内 (NVLink) vs. 节点间 (InfiniBand) 带宽差异大 (例: 4:1)。

</div>

<div v-click="2">

- **目标**: 规避网络瓶颈。

</div>

---
page: 20

layout: two-cols
subtitles: {"default":{"zh_CN":["节点限制路由 (Node-Limited Routing) 策略：","这是DeepSeek-V3针对上述带宽差异做出的一项关键协同设计决策。"],"en":["Node-Limited Routing Strategy:","This is a key co-design decision made by DeepSeek-V3 in response to the aforementioned bandwidth difference."]},"click1":{"zh_CN":["该策略在MoE架构的专家选择过程中，","通过算法确保每个token最多被路由到4个节点（总共可能有8个节点组）上的专家。","这样就尽量避免了走“慢车道”，/D/1000"],"en":["This strategy, during the expert selection process in the MoE architecture,","Ensures through algorithms that each token is routed to experts on at most 4 nodes (out of a total of 8 node groups).","This minimizes using the 'slow lane',/D/1000"]},"click2":{"zh_CN":["这体现了针对带宽差异的软件适应。","通过软件层面的策略来适应硬件的限制。"],"en":["This reflects a software adaptation targeting the bandwidth difference.","Using software-level strategies to adapt to hardware limitations."]},"click3":{"zh_CN":["它的优势在于：","减少跨节点通信的开销。","更有效地利用了节点内的高带宽互连。","基于特定硬件互连的限制优化了整体性能。"],"en":["Its advantages include:","Reducing cross-node communication overhead.","More effectively utilizing high bandwidth interconnects within nodes.","Optimizing overall performance based on specific hardware interconnect limitations."]},"click4":{"zh_CN":["节点限制路由 可以视为一种对硬件现实（带宽不均衡）的巧妙软件适应。","它并非追求理想状态下（但成本高昂且难以大规模实现）的完美、统一的跨节点带宽，","而是约束模型的路由逻辑，使其在现有硬件限制下工作，优先利用更快的本地通信。","这正是协同设计带来实际性能提升的典范。"],"en":["Node-Limited Routing can be seen as a clever software adaptation to hardware realities (uneven bandwidth).","It does not pursue perfect, uniform cross-node bandwidth in an ideal state (which is costly and difficult to achieve at scale),","Instead, it constrains the model's routing logic to work within existing hardware limitations, prioritizing faster local communication.","This is a prime example of how co-design brings tangible performance improvements."]}}
---

## 节点限制路由 (Node-Limited Routing)

<div v-click="1">

- **策略**: MoE专家选择时，token最多路由到**4个节点** (总共8个节点组)。

</div>

<div v-click="2">

- **协同设计**: 针对带宽差异的软件适应。

</div>

<div v-click="3">

- **优势**: 减少跨节点通信开销，更有效地利用节点内高带宽。

</div>

<div v-click="4">

节点限制路由 8 可以视为一种对硬件现实（带宽不均衡）的巧妙软件适应。
它并非追求理想状态下（但成本高昂且难以大规模实现）的完美、统一的跨节点带宽，而是约束模型的路由逻辑，使其在现有硬件限制下工作，优先利用更快的本地通信。
这正是协同设计带来实际性能提升的典范。

</div>

::right::



---
page: 21

layout: two-cols
subtitles: {"default":{"zh_CN":["多平面胖树 (MPFT) 网络架构：","为了构建可扩展且高效的横向扩展（scale-out）网络基础设施，","DeepSeek-V3的训练过程采用了一种多平面胖树（Multi-Plane Fat-Tree, MPFT）网络架构。"],"en":["Multi-Plane Fat-Tree (MPFT) Network Architecture:","To build a scalable and efficient scale-out network infrastructure,","DeepSeek-V3's training process adopts a Multi-Plane Fat-Tree (MPFT) network architecture."]},"click1":{"zh_CN":["概念：","MPFT是一种网络拓扑，","其中每个计算节点的多个GPU和对应的多个网络接口卡（NIC）","被分配到不同的、逻辑上独立的网络“平面”或胖树结构中。"],"en":["Concept:","MPFT is a network topology,","where multiple GPUs and corresponding multiple Network Interface Cards (NICs) of each compute node","Are assigned to different, logically independent network 'planes' or fat-tree structures."]},"click2":{"zh_CN":["DeepSeek-V3实现：","训练集群利用了MPFT横向扩展网络。","每个节点包含8个GPU和8个InfiniBand NIC，","每个GPU-NIC对被分配到一个独立的网络平面。","这意味着每个GPU-NIC对连接到其各自平面内的不同叶子交换机（leaf switch）。"],"en":["DeepSeek-V3 Implementation:","The training cluster utilizes the MPFT scale-out network.","Each node contains 8 GPUs and 8 InfiniBand NICs,","And each GPU-NIC pair is assigned to an independent network plane.","This means each GPU-NIC pair connects to a different leaf switch within its respective plane."]},"click3":{"zh_CN":["优势：","提供了一种经济高效的横向扩展网络方案。","通过提供多条独立的通信路径，增加了聚合带宽并提高了网络的容错能力。","与非“轨道优化”（rail-optimized，NVIDIA对类似概念的称呼）配置相比，","允许将更多的GPU节点连接到同一组交换机设备上。"],"en":["Advantages:","Provides a cost-effective horizontal scaling network solution.","By providing multiple independent communication paths, it increases aggregate bandwidth and improves network fault tolerance.","Compared to non-'rail-optimized' configurations (NVIDIA's term for a similar concept),","It allows more GPU nodes to be connected to the same set of switch devices."]},"click4":{"zh_CN":["理想实现（如DeepSeek论文中所述）：","NIC具有多个物理端口，分别连接到不同的网络平面，","但对上层软件暴露为单个逻辑接口，","拥有单个队列对（queue pair）可以同时访问所有端口。"],"en":["Ideal Implementation (as described in the DeepSeek paper):","The NIC has multiple physical ports connected to different network planes respectively,","But exposed to upper-layer software as a single logical interface,","With a single queue pair that can access all ports simultaneously."]},"click5":{"zh_CN":["MPFT网络架构 的意义不仅在于增加链路数量，","更在于通过结构化设计，实现大规模场景下无阻塞或近乎无阻塞的通信，并提供故障恢复能力。","通过将GPU-NIC对专用于不同平面，DeepSeek-V3创建了并行的、很大程度上独立的通信路径，","这对于LLM训练中常见的全归约（all-reduce）和全收集（all-gather）等集体通信模式至关重要。"],"en":["The significance of the MPFT network architecture is not just in increasing the number of links,","But more importantly, in achieving non-blocking or nearly non-blocking communication in large-scale scenarios through structural design, and providing fault tolerance.","By dedicating GPU-NIC pairs to different planes, DeepSeek-V3 creates parallel, largely independent communication paths,","Which is crucial for collective communication patterns common in LLM training, such as all-reduce and all-gather."]}}
---

## 多平面胖树 (MPFT) 网络架构

<div v-click="1">

- **概念**: 计算节点的GPU/NIC分配到不同、独立的网络“平面”。

</div>

<div v-click="2">

- **DeepSeek实现**: 每个节点8GPU+8IB NIC，每对分到独立平面。

</div>

<div v-click="3">

- **优势**: 经济高效横向扩展，增加聚合带宽，提高容错能力。

</div>

<div v-click="4">

- **理想**: NIC多端口连接不同平面，上层软件统一访问。

</div>

<div v-click="5">

MPFT网络架构 的意义不仅在于增加链路数量，更在于通过结构化设计，实现大规模场景下无阻塞或近乎无阻塞的通信，并提供故障恢复能力。
通过将GPU-NIC对专用于不同平面，DeepSeek-V3创建了并行的、很大程度上独立的通信路径，这对于LLM训练中常见的全归约（all-reduce）和全收集（all-gather）等集体通信模式至关重要。

</div>

::right::



---
page: 22

layout: two-cols
subtitles: {"default":{"zh_CN":["5.3. 低延迟网络考量 (InfiniBand vs. RoCE)","DeepSeek-V3的训练集群使用了InfiniBand (IB) 作为其主要的低延迟、高带宽互连技术。"],"en":["5.3. Low-Latency Network Considerations (InfiniBand vs. RoCE)","DeepSeek-V3's training cluster uses InfiniBand (IB) as its primary low-latency, high-bandwidth interconnect technology."]},"click1":{"zh_CN":["然而，其研究论文也探讨了RoCE (RDMA over Converged Ethernet) 作为替代方案的可能性，","并针对RoCE在AI负载下的表现提出了一些改进建议，","这表明团队对网络技术的未来发展保持关注：","专用低延迟RoCE交换机：针对RDMA工作负载进行优化的交换机。","优化的路由策略：使用自适应路由代替等价多路径（ECMP）路由。","改进的流量隔离：通过虚拟输出队列（VOQ）或更高级的拥塞控制机制。"],"en":["However, its research paper also explores the possibility of RoCE (RDMA over Converged Ethernet) as an alternative,","And proposes some improvement suggestions for RoCE's performance under AI workloads,","This indicates that the team is paying attention to the future development of network technology:","Dedicated low-latency RoCE switches: Switches optimized for RDMA workloads.","Optimized routing strategies: Using adaptive routing instead of Equal-Cost Multi-Path (ECMP) routing.","Improved traffic isolation: Through Virtual Output Queues (VOQ) or more advanced congestion control mechanisms."]},"click2":{"zh_CN":["论文的目录中还提及了InfiniBand GPUDirect Async (IBGDA)，","这是一种允许GPU直接异步访问IB网络的技术，","可以进一步降低通信延迟。"],"en":["The paper's table of contents also mentions InfiniBand GPUDirect Async (IBGDA),","Which is a technology that allows GPUs to access the IB network directly and asynchronously,","It can further reduce communication latency."]},"click3":{"zh_CN":["尽管DeepSeek-V3使用了InfiniBand，但其对RoCE的讨论和改进建议体现了一种前瞻性视角。","这不仅仅是为当前硬件进行优化，","也旨在为未来硬件的发展提供洞见和“建议”，","从而促进AI模型与基础设施的共同进化。"],"en":["Although DeepSeek-V3 uses InfiniBand, its discussion on RoCE and improvement suggestions reflect a forward-looking perspective.","This is not just optimizing for current hardware,","It also aims to provide insights and 'suggestions' for the future development of hardware,","Thereby promoting the co-evolution of AI models and infrastructure."]}}
---

## 低延迟网络考量

- DeepSeek-V3 使用 InfiniBand (IB)。

<div v-click="1">

- 研究探索 RoCE (RDMA over Converged Ethernet) 作为替代。
    -   专用低延迟RoCE交换机
    -   优化的路由策略
    -   改进流量隔离

</div>

<div v-click="2">

- 提及 IB GPUDirect Async (IBGDA) 降低延迟。

</div>

<div v-click="3">

- 前瞻性视角：为未来硬件发展提供洞见。

</div>

::right::



---
page: 23

layout: two-cols
subtitles: {"default":{"zh_CN":["6. 综合硬件-模型协同设计：反思与未来方向/D/1000","6.1. DeepSeek-V3：整体优化的典范/D/1000"],"en":["6. Integrated Hardware-Model Co-design: Reflection and Future Directions/D/1000","6.1. DeepSeek-V3: A Paradigm of Holistic Optimization/D/1000"]},"click1":{"zh_CN":["DeepSeek-V3的开发过程充分展示了硬件-模型协同设计的力量。","通过MLA对内存的精巧管理、","MoE架构（特别是其新颖的负载均衡和细粒度专家设计）对计算成本的有效控制、","FP8与LogFMT对数据表示和传输的极致压缩、","节点限制路由对网络瓶颈的规避，","以及MPFT网络架构对系统扩展性的保障，","这些技术并非孤立存在，而是作为一个有机的整体，共同应对了LLM扩展所面临的内存、计算和通信三重困境。"],"en":["The development process of DeepSeek-V3 fully demonstrates the power of hardware-model co-design.","Through MLA's sophisticated management of memory,","MoE architecture (especially its novel load balancing and fine-grained expert design) for effective control of computational costs,","FP8 and LogFMT for extreme compression of data representation and transmission,","Node-limited routing for avoiding network bottlenecks,","And MPFT network architecture for ensuring system scalability,","These technologies do not exist in isolation but as an organic whole, jointly addressing the core scaling challenges of LLMs in memory, computation, and communication."]},"click2":{"zh_CN":["它们共同应对了内存、计算、通信这“三重困境”。"],"en":["They collectively address the 'triple dilemma' of memory, computation, and communication."]},"click3":{"zh_CN":["其核心在于：","设计决策仔细对齐硬件约束。"],"en":["The core lies in:","Design decisions are carefully aligned with hardware constraints."]},"click4":{"zh_CN":["成果是：","在有限硬件资源（2048块NVIDIA H800 GPU）上，","高效训练和运行了大规模模型（671B参数）的目标。"],"en":["The result is:","On limited hardware resources (2048 NVIDIA H800 GPUs),","Achieving the goal of efficiently training and running large-scale models (671B parameters)."]}}
---

## 综合硬件-模型协同设计

### DeepSeek-V3：整体优化的典范

<div v-click="1">

- **集成方法**: MLA, MoE, FP8, LogFMT, 节点限制路由, MPFT。

</div>

<div v-click="2">

- **共同应对**: 内存、计算、通信“三重困境”。

</div>

<div v-click="3">

- **核心**: 设计决策仔细对齐硬件约束。

</div>

<div v-click="4">

- **成果**: 在有限资源 (2048 H800) 上高效训练大规模模型 (671B参数)。

</div>

::right::



---
page: 24

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["6.2. 遭遇的关键硬件瓶颈与反思/D/1000"],"en":["6.2. Encountered Key Hardware Bottlenecks and Reflection/D/1000"]},"click1":{"zh_CN":["DeepSeek-V3的设计选择旨在缓解一系列硬件限制，包括但不限于：","内存容量不足、","内存带宽瓶颈、","以及节点间互连带宽的相对匮乏。"],"en":["DeepSeek-V3's design choices aim to alleviate a range of hardware limitations, including but not limited to:","Insufficient memory capacity,","Memory bandwidth bottlenecks,","And the relative scarcity of inter-node interconnect bandwidth."]},"click2":{"zh_CN":["通过这些实践，研究团队积累了宝贵的经验。"],"en":["Through these practices, the research team has accumulated valuable experience."]},"click3":{"zh_CN":["并期望能“与学术界和工业界的同行","就未来硬件的潜在发展方向展开更广泛的讨论”。","这种从实践中来到实践中去，","并积极反哺硬件设计思路的循环，","是推动整个AI生态发展的关键。"],"en":["And hopes to “engage in broader discussions with colleagues from academia and industry","on the potential future development directions of hardware”.","This cycle of coming from practice and returning to practice,","And actively feeding back into hardware design thinking,","Is crucial for promoting the development of the entire AI ecosystem."]}}
---

## 关键硬件瓶颈与反思

<div v-click="1">

- **遇到的限制**: 内存容量、内存带宽、节点间互连带宽相对匮乏。

</div>

<div v-click="2">

- **实践经验**: 积累宝贵经验。

</div>

<div v-click="3">

- **期望**: 与同行就未来硬件发展方向展开更广泛讨论。

</div>

---
page: 25

layout: two-cols
subtitles: {"default":{"zh_CN":["6.3. 对未来硬件架构的启示/D/1000"],"en":["6.3. Implications for Future Hardware Architectures/D/1000"]},"click1":{"zh_CN":["基于DeepSeek-V3的开发经验和论文中的讨论，","可以为未来AI硬件架构的设计提供如下启示：","精确的低精度计算单元：FP8的成功应用表明，硬件不仅需要支持低精度计算，","更要保证其计算结果的准确性，并最小化数据类型转换和缩放操作带来的开销。","纵向扩展（Scale-Up）与横向扩展（Scale-Out）的融合：","NVLink与InfiniBand之间的带宽差异揭示了对节点内和节点间通信带宽更无缝、更均衡的需求，","或者需要硬件/软件能更好地抽象或管理这种差异。","低延迟、高带宽互连技术的创新：","对更优网络接口和拓扑结构（如MPFT的进一步发展）的需求持续存在。","以内存为中心的创新 (Memory-Centric Innovations)：","解决“内存墙”问题仍然至关重要。","这可能包括更高带宽的内存技术、近内存计算、或更智能的内存层次结构。","面向AI的智能网络 (Intelligent Networks for AI)：","网络设备应能更主动地参与计算或数据管理，","例如支持“网络内计算与压缩” (In-Network Computation and Compression)。"],"en":["Based on DeepSeek-V3's development experience and discussions in the paper,","The following insights can be provided for the design of future AI hardware architectures:","Precise Low-Precision Computing Units: The successful application of FP8 indicates that hardware not only needs to support low-precision computation,","But also needs to ensure the accuracy of its computation results and minimize the overhead brought by data type conversion and scaling operations.","Fusion of Scale-Up and Scale-Out:","The bandwidth difference between NVLink and InfiniBand reveals a demand for more seamless and balanced intra-node and inter-node communication bandwidth,","Or requires hardware/software to better abstract or manage this difference.","Innovation in Low-Latency, High-Bandwidth Interconnect Technologies:","The need for better network interfaces and topological structures (such as the further development of MPFT) continues to exist.","Memory-Centric Innovations:","Solving the 'memory wall' problem remains crucial.","This may include higher bandwidth memory technologies, near-memory computing, or more intelligent memory hierarchies.","Intelligent Networks for AI:","Network devices should be able to participate more actively in computation or data management,","For example, supporting 'In-Network Computation and Compression'."]},"click2":{"zh_CN":["如幻灯片上图表所示，这是主要的扩展挑战","以及DeepSeek-V3中相应的应对方法和对未来硬件的启示。"],"en":["As shown in the table on the slide, these are the main scaling challenges","And the corresponding solutions in DeepSeek-V3 and implications for future hardware."]},"click3":{"zh_CN":["这里是这些启示的一个总结表格。"],"en":["Here is a summary table of these implications."]}}
---

## 对未来硬件架构的启示

如幻灯片上图表所示，DeepSeek-V3的经验提供了如下启示：

<div v-click="1">

基于DeepSeek-V3的开发经验和论文中的讨论 1，可以为未来AI硬件架构的设计提供如下启示：

- 精确的低精度计算单元：FP8的成功应用表明，硬件不仅需要支持低精度计算，更要保证其计算结果的准确性，并最小化数据类型转换和缩放操作带来的开销。
- 纵向扩展（Scale-Up）与横向扩展（Scale-Out）的融合：NVLink与InfiniBand之间的带宽差异 8 揭示了对节点内和节点间通信带宽更无缝、更均衡的需求，或者需要硬件/软件能更好地抽象或管理这种差异。
- 低延迟、高带宽互连技术的创新：对更优网络接口和拓扑结构（如MPFT的进一步发展）的需求持续存在。
- 以内存为中心的创新 (Memory-Centric Innovations)：解决“内存墙”问题仍然至关重要 2。
    - 这可能包括更高带宽的内存技术、近内存计算、或更智能的内存层次结构。
- 面向AI的智能网络 (Intelligent Networks for AI)：网络设备应能更主动地参与计算或数据管理，例如支持“网络内计算与压缩” (In-Network Computation and Compression) 2。

</div>

<div v-click="2">

| 扩展挑战              | DeepSeek-V3方法                 | 未来硬件启示                   |
| :-------------------- | :------------------------------ | :----------------------------- |
| KV缓存内存膨胀        | MLA                             | 以内存为中心的创新             |
| 高计算成本            | MoE                             | 先进MoE支持                    |
| 模型参数内存占用      | FP8混合精度训练                 | 精确低精度计算单元             |
| 通信数据量            | LogFMT通信压缩 (FP8)            | 面向AI的智能网络 (压缩/计算)   |
| MoE跨节点带宽瓶颈     | 节点限制路由                    | 纵向/横向扩展融合，管理带宽差 |
| 网络可扩展性/弹性性   | 多平面胖树网络                  | 低延迟、高带宽互连创新         |

</div>

<div v-click="3">

如幻灯片上图表所示，这是主要的扩展挑战以及DeepSeek-V3中相应的应对方法和对未来硬件的启示。

</div>

::right::



---
page: 26

layout: two-cols
subtitles: {"default":{"zh_CN":["协同进化的力量/D/1000"],"en":["The Power of Co-Evolution/D/1000"]},"click1":{"zh_CN":["DeepSeek-V3的论文不仅是对一个模型的描述，","更是与硬件社区的一场对话。"],"en":["DeepSeek-V3's paper is not just a description of a model,","It is more of a dialogue with the hardware community."]},"click2":{"zh_CN":["这暗示了一个迭代过程：","模型构建者将现有硬件推向极限，识别新的瓶颈，","并为下一代硬件提出特性建议，","而新一代硬件的出现又将催生新的模型架构。","这种模型与硬件的协同进化，是推动AI技术持续突破的核心动力。"],"en":["This suggests an iterative process:","Model builders push existing hardware to its limits, identify new bottlenecks,","And propose feature suggestions for the next generation of hardware,","And the emergence of a new generation of hardware will in turn give rise to new model architectures.","This co-evolution of models and hardware is the core driving force behind the continuous breakthroughs in AI technology."]},"click3":{"zh_CN":["DeepSeek-V3以相对较少的GPU资源（2048块H800）","成功训练出具有竞争力的671B参数模型，","突显了从纯粹依赖大规模资源投入（可能需要数万块GPU）","向通过架构和协同设计创新实现“更智能”扩展的转变。"],"en":["DeepSeek-V3 successfully trained a competitive 671B parameter model","with relatively few GPU resources (2048 H800s),","Highlighting the shift from purely relying on large-scale resource investment (potentially requiring tens of thousands of GPUs)","towards achieving 'smarter' scaling through architecture and co-design innovation."]},"click4":{"zh_CN":["这使得SOTA LLM的开发更具普适性。","许多DeepSeek-V3的创新（如节点限制路由、LogFMT、特定的MLA/MoE设计）","本质上是通过软件/算法方案来缓解固有的硬件限制（如带宽差异、通信成本）。","这强调了软件和算法的创造力","在最大化现有硬件效用方面所发挥的关键作用。"],"en":["This makes the development of SOTA LLMs more universal.","Many of DeepSeek-V3's innovations (such as node-limited routing, LogFMT, specific MLA/MoE designs)","Essentially alleviate inherent hardware limitations (like bandwidth differences, communication costs) through software/algorithmic solutions.","This emphasizes the creativity of software and algorithms","And the key role they play in maximizing the utility of existing hardware."]}}
---

## 协同进化的力量

<div v-click="1">

- DeepSeek-V3论文：与硬件社区的对话。

</div>

<div v-click="2">

- **迭代过程**: 模型推硬件极限 -> 识别瓶颈 -> 建议新特性 -> 新硬件 -> 新模型架构。

</div>

<div v-click="3">

- **关键转变**: 从依赖大规模资源投入到通过协同设计实现“更智能”扩展。

</div>

<div v-click="4">

- **意义**: 降低SOTA LLM开发门槛，增强普适性。
许多DeepSeek-V3的创新（如节点限制路由、LogFMT、特定的MLA/MoE设计）本质上是通过软件/算法方案来缓解固有的硬件限制（如带宽差异、通信成本）。
这强调了软件和算法的创造力在最大化现有硬件效用方面所发挥的关键作用。

</div>

::right::



---
page: 27

layout: two-cols
subtitles: {"default":{"zh_CN":["7. 结论：DeepSeek-V3的贡献与AI系统的未来之路/D/1000","7.1. DeepSeek-V3方法论影响总结/D/1000"],"en":["7. Conclusion: DeepSeek-V3's Contribution and the Future of AI Systems/D/1000","7.1. Summary of DeepSeek-V3 Methodology Impact/D/1000"]},"click1":{"zh_CN":["DeepSeek-V3通过一系列精心设计和协同优化的方法，","包括多头隐注意力（MLA）、","细粒度混合专家（MoE）及其无辅助损失负载均衡、","FP8混合精度训练、","LogFMT通信压缩、","节点限制路由和多平面胖树网络架构，","成功地应对了大规模语言模型在内存、计算和通信方面的核心扩展挑战。"],"en":["DeepSeek-V3, through a series of meticulously designed and co-optimized methods,","Including Multi-Head Latent Attention (MLA),","Fine-grained Mixture-of-Experts (MoE) and its auxiliary-loss-free load balancing,","FP8 mixed-precision training,","LogFMT communication compression,","Node-limited routing and Multi-Plane Fat-Tree network architecture,","Successfully addressed the core scaling challenges of large language models in memory, computation, and communication."]},"click2":{"zh_CN":["这些方法的集成应用，取得了显著成果，","高效应对了内存、计算、通信挑战。"],"en":["The integrated application of these methods has achieved significant results,","Effectively addressing memory, computation, and communication challenges."]},"click3":{"zh_CN":["这带来了深远的影响：","提升了效率，","并显著降低了成本。"],"en":["This has had profound impacts:","Improving efficiency,","And significantly reducing costs."]},"click4":{"zh_CN":["核心价值在于：","DeepSeek-V3的实践雄辩地证明了硬件-模型协同设计","在构建下一代高性能、高效率AI系统中的核心价值和巨大潜力。"],"en":["The core value lies in:","DeepSeek-V3's practice eloquently demonstrates the hardware-model co-design's","Core value and immense potential in building next-generation high-performance, high-efficiency AI systems."]}}
---

## 结论：DeepSeek-V3的贡献

<div v-click="1">

- **方法论总结**: MLA, MoE, FP8, LogFMT, 节点限制路由, MPFT。

</div>

<div v-click="2">

- **成果**: 高效应对内存、计算、通信挑战。

</div>

<div v-click="3">

- **影响**: 提升效率，降低成本。

</div>

<div v-click="4">

- **核心价值**: 硬件-模型协同设计。

</div>

::right::



---
page: 28

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["7.2. 对LLM研发的广泛启示/D/1000"],"en":["7.2. Broad Implications for LLM R&D/D/1000"]},"click1":{"zh_CN":["DeepSeek-V3的成果对整个大规模语言模型研究和开发领域具有深远的启示：","成本效益的提升：它展示了通过智能设计而非仅仅依赖资源堆砌，","也能够开发出具有竞争力的大型模型，","这为资源相对有限的研究机构和企业开辟了新的可能性。"],"en":["DeepSeek-V3's achievements have profound implications for the entire field of large language model research and development:","Improved Cost-Effectiveness: It demonstrates that through intelligent design rather than merely relying on resource accumulation,","Competitive large models can also be developed,","Opening up new possibilities for research institutions and companies with relatively limited resources."]},"click2":{"zh_CN":["整体系统观的重要性：","强调了在LLM开发中必须采取整体系统观，","将算法、软件栈和硬件基础设施视为一个相互关联、相互影响的统一体","进行综合考量和联合优化。"],"en":["Importance of a Holistic System View:","Emphasizes the necessity of adopting a holistic system view in LLM development,","Considering algorithms, software stacks, and hardware infrastructure as an interconnected, mutually influential unified entity","For comprehensive consideration and joint optimization."]},"click3":{"zh_CN":["创新方向的指引：","DeepSeek-V3中采用的各项具体技术，","如MLA对注意力机制的改进、MoE对计算模式的革新、低精度技术对资源占用的压缩等，","都为后续LLM的架构创新指明了有前景的方向。"],"en":["Guidance for Innovation Directions:","The specific technologies adopted in DeepSeek-V3,","Such as MLA's improvement on attention mechanisms, MoE's innovation in computation patterns, and low-precision technology's compression of resource usage, etc.,","All point to promising directions for subsequent LLM architectural innovation."]}}
---

## 对LLM研发的广泛启示

<div v-click="1">

- **成本效益**: 智能设计而非资源堆砌也能实现竞争力。

</div>

<div v-click="2">

- **整体系统观**: 算法、软件、硬件需联合优化。

</div>

<div v-click="3">

- **指引创新**: MLA, MoE, 低精度等技术指明方向。

</div>

---
page: 29

layout: two-cols
subtitles: {"default":{"zh_CN":["7.3. 未来研究轨迹展望/D/1000"],"en":["7.3. Future Research Trajectory Outlook/D/1000"]},"click1":{"zh_CN":["未来，DeepSeek-V3的探索为AI系统的进一步发展奠定了基础，","并揭示了若干值得深入研究的轨迹：","低精度技术的深化：","继续探索更低位宽（如FP4、INT4甚至二值/三值网络）的可行性、鲁棒性及其在训练和推理中的高效应用方法，","同时开发能够更好支持这些超低精度运算的硬件。"],"en":["In the future, DeepSeek-V3's exploration lays the foundation for the further development of AI systems,","And reveals several trajectories worth in-depth research:","Deepening of Low-Precision Technologies:","Continue to explore the feasibility, robustness, and efficient application methods of lower bit widths (such as FP4, INT4, or even binary/ternary networks) in training and inference,","While developing hardware that can better support these ultra-low precision operations."]},"click2":{"zh_CN":["先进的MoE策略：","研究更动态、更自适应的专家路由算法和负载均衡机制，","探索专家能力的自动发现与组合，","以及如何在MoE架构中更有效地实现知识迁移和持续学习。"],"en":["Advanced MoE Strategies:","Research into more dynamic and adaptive expert routing algorithms and load balancing mechanisms,","Exploring the automatic discovery and combination of expert capabilities,","And how to more effectively implement knowledge transfer and continuous learning in MoE architectures."]},"click3":{"zh_CN":["新型注意力机制的探索：","超越现有注意力范式，","探索具备更优扩展性（如线性复杂度）、","更长上下文处理能力且保持高性能的新型信息交互机制。"],"en":["Exploration of Novel Attention Mechanisms:","Going beyond existing attention paradigms,","Exploring new information interaction mechanisms with better scalability (such as linear complexity),","Longer context handling capabilities and maintaining high performance."]},"click4":{"zh_CN":["硬件与模型的持续协同进化：","深化模型架构与专用AI芯片（如NPU、TPU的下一代）的协同设计，","探索存内计算、光互连、3D堆叠等新兴硬件技术在LLM加速中的应用潜力。"],"en":["Continuous Co-evolution of Hardware and Models:","Deepening the co-design of model architectures with dedicated AI chips (such as the next generation of NPUs, TPUs),","Exploring the application potential of emerging hardware technologies like in-memory computing, optical interconnects, and 3D stacking in LLM acceleration."]}}
---

## 未来研究轨迹展望

<div v-click="1">

- **低精度深化**: FP4/INT4，超低精度硬件支持。

</div>

<div v-click="2">

- **先进MoE**: 动态路由，自适应负载均衡，知识迁移。

</div>

<div v-click="3">

- **新型注意力**: 线性复杂度，更长上下文，高性能。

</div>

<div v-click="4">

- **硬件模型协同**: 专用AI芯片，存内计算，光互连，3D堆叠。

</div>

::right::



---
page: 30

layout: image-left
image: "https://cover.sli.dev"
subtitles: {"default":{"zh_CN":["DeepSeek-V3不仅是一个高性能的LLM，","更提供了一份“下一代AI系统创新的实用蓝图”。"],"en":["DeepSeek-V3 is not only a high-performance LLM,","But also provides a 'practical blueprint for next-generation AI system innovation'."]},"click1":{}}
---

## DeepSeek-V3：下一代AI系统的实用蓝图

<div v-click="1">

- 不仅是模型，更是创新实践案例。

</div>

<div v-click="2">

- 协同设计哲学影响深远。

</div>

<div v-click="3">

- 对“经济高效”的追求与可持续AI发展方向一致。

</div>